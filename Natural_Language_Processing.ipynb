{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverTwilight/Awesome-Machine-Learning-Playground/blob/master/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDTIwtCYALv7"
      },
      "source": [
        "# Overview\n",
        "\n",
        "> By Rene Wang\n",
        "\n",
        "This notebook provides a comprehensive explanation of how a basic language model operates, covering all the essential steps. Upon completion, you will possess a solid foundational understanding of Natural Language Processing (NLP), enabling you to delve deeper into the subject.\n",
        "\n",
        "Note: It's recommend to read the [Handwritten_Digits_Detection]() first. Some basic concepts and codes are explained there.\n",
        "\n",
        "*If you encounter problems while studying this notebook, feel free to submit an issue in this [repo](https://github.com/RiverTwilight/Awesome-Machine-Learning-Playground)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-XZlvzavc55"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import jieba\n",
        "import time\n",
        "import pickle\n",
        "!pip install -q kora colorama\n",
        "from colorama import Fore, Back, Style\n",
        "from kora import drive\n",
        "drive.link_nbs()\n",
        "from Handwritten_Digits_Recognition import SoftmaxWithLoss, Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iL0SC9k2rdr"
      },
      "source": [
        "To render Chinese character in matplotlib we need to download the font first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ix4iu8ucmch",
        "outputId": "76d159cc-f45a-4d5a-f877-8d0b421d6d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-12 07:48:39--  https://github.com/adobe-fonts/source-han-sans/blob/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf [following]\n",
            "--2023-06-12 07:48:39--  https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/adobe-fonts/source-han-sans/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf [following]\n",
            "--2023-06-12 07:48:39--  https://raw.githubusercontent.com/adobe-fonts/source-han-sans/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16414944 (16M) [application/octet-stream]\n",
            "Saving to: ‘SourceHanSansSC-Normal.otf’\n",
            "\n",
            "SourceHanSansSC-Nor 100%[===================>]  15.65M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-06-12 07:48:40 (208 MB/s) - ‘SourceHanSansSC-Normal.otf’ saved [16414944/16414944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O SourceHanSansSC-Normal.otf https://github.com/adobe-fonts/source-han-sans/blob/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf?raw=true\n",
        "\n",
        "os.makedirs('/root/.config/matplotlib', exist_ok=True)\n",
        "\n",
        "!cp SourceHanSansSC-Normal.otf /root/.config/matplotlib/\n",
        "\n",
        "matplotlib.font_manager.fontManager.addfont('SourceHanSansSC-Normal.otf')\n",
        "\n",
        "matplotlib.rc(\"font\",family='Source Han Sans SC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXyinmCk9vVe"
      },
      "source": [
        "# Word Embedding: Count-based Method\n",
        "\n",
        "In order to put the complex the data into neural network, the most-used solution is to convert the data to vector space, which is called **word2vec**. In this part we'll discuss how to convert the natural text data into vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_W945-M3fon"
      },
      "source": [
        "## Word co-occurrence matrix\n",
        "\n",
        "Usually the meaning of a word depends on the context. A word co-occurrence matrix reprents how many times has a word appear around another word.\n",
        "\n",
        "Consider this sentence:\n",
        "```markdown\n",
        " You are my friend and I am his friend.\n",
        "```\n",
        "First we remove the duplicate words and we can create a table like this:\n",
        "\n",
        "\\begin{array}{ccc}\n",
        "\\text{ }&\\text{You}&\\text{are}&\\text{my}&\\text{friend}&\\text{and}&\\text{I}&\\text{am}&\\text{his}&\\text{.}\\\\\n",
        "\\text{You}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{are}&\\text{1}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{my}&\\text{1}&\\text{1}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{friend}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{and}&\\text{0}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{I}&\\text{0}&\\text{0}&\\text{0}&\\text{1}&\\text{1}&\\text{0}&\\text{1}&\\text{1}&\\text{0}\\\\\n",
        "\\text{am}\\\\\n",
        "\\text{his}\\\\\n",
        "\\end{array}\n",
        "\n",
        "The row label reprents those words exsit in the context of the col lable. The context size was called **window_size**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSGRJnCBmsae"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('Use Python3!')\n",
        "import pickle\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Project/NLP'\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = dataset_dir + '/' + vocab_file\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def load_ptb_data(data_type='train'):\n",
        "    '''\n",
        "        :param data_type: 数据的种类：'train' or 'test' or 'valid (val)'\n",
        "        :return:\n",
        "    '''\n",
        "    if data_type == 'val': data_type = 'valid'\n",
        "    save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "    word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        corpus = np.load(save_path)\n",
        "        return corpus, word_to_id, id_to_word\n",
        "\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    np.save(save_path, corpus)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "def postprocess(text, split_policy=\"Default\"):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', \" .\")\n",
        "\n",
        "    if split_policy == \"Chinese\":\n",
        "        words = list(jieba.cut(text))\n",
        "    else:\n",
        "        words = text.split(' ')\n",
        "\n",
        "    print(words)\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "\n",
        "    for word in words:\n",
        "        new_id = len(word_to_id)\n",
        "        word_to_id.setdefault(word, new_id)\n",
        "        id_to_word.setdefault(new_id, word)\n",
        "\n",
        "    corpus = [word_to_id[w] for w in words]\n",
        "    corpus = np.array(corpus)\n",
        "\n",
        "    return corpus, word_to_id, id_to_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU7QRZBxps3S"
      },
      "outputs": [],
      "source": [
        "def create_co_matrix(corpus, vocab_size, windows_size=1):\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(windows_size):\n",
        "            left_idx = idx - 1\n",
        "            right_idx = idx + 1\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FImNwpIcry0e"
      },
      "source": [
        "## PPMI & SVD\n",
        "\n",
        "But this doesn't represnet the link between word very well. Thus we introduced the PMI. The $P(x)$ denote the probablity of the event x.\n",
        "\n",
        "$$ PMI(x, y) = log_{2}\\frac{P(x, y)}{P(x)P(y)} $$\n",
        "\n",
        "In case the appearance of negative values, we introduced the PPMI (Positive Pointwise Mutual Information). While negative PMI values can provide useful information, in many applications, only the strength of the positive association is of interest. The PPMI measure clips the PMI at zero, turning all negative PMI values to zero:\n",
        "\n",
        "$$ PPMI(X,Y) = max(PMI(X,Y), 0) $$\n",
        "\n",
        "But as the volcabulary grows, use a 2-dimension array like this is not efficient. So we introduced the Singular Value Decomposition, or SVD, to reduce the dimension.\n",
        "\n",
        "$$ X=USV^{t} $$\n",
        "\n",
        "In the graph, the nearer the two word are, it means the similerer the two words are. Following cell will implement PPMI & SVD to the matrix.\n",
        "\n",
        "![SVD](https://pic3.zhimg.com/v2-f249e8a4be916e51b9d537c8380ae6e2_b.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APd_lfTcr9pf"
      },
      "outputs": [],
      "source": [
        "def ppmi(C, verbose=False, eps=1e-8):\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100+1) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "\n",
        "    return M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnEOqnlj20k0"
      },
      "source": [
        "## Similarity Comparsion\n",
        "\n",
        "Our goal is to make the AI know the word based on their meaning. So we have to calculate the similarity between the words.\n",
        "\n",
        "For example, we want the machine know that \"I\", \"You\" and \"He\" are simliary words in the English. This part we will **calculate the similarity of vectors**.\n",
        "\n",
        "We have servals alogorithum to finish the tasks:\n",
        "\n",
        "* Euclidean Distance\n",
        "* Cosine Similarity\n",
        "* Manhattan Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzCooXjkq2By"
      },
      "outputs": [],
      "source": [
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
        "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    if query not in word_to_id:\n",
        "        print('%s is not found' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHMPpGuZvBym"
      },
      "source": [
        "# Word-embedding: Predict-based Method\n",
        "\n",
        "By using this method, we can get a more-connected word-emnbedding. This method is also called word2vec, which foucus on predict the word with given context or vise versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzUu0gq9qqNz"
      },
      "source": [
        "## Context Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t4eI4X-qyiP"
      },
      "outputs": [],
      "source": [
        "def create_contexts_target(corpus, window_size=1):\n",
        "    target = corpus[window_size: -window_size]\n",
        "    context = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        context.append(cs)\n",
        "\n",
        "    return np.array(context), np.array(target)\n",
        "\n",
        "def convert_one_hot(contexts, vocab_size):\n",
        "    one_hots = []\n",
        "\n",
        "    for context in contexts:\n",
        "        if type(context) is np.ndarray:\n",
        "            one_hots.append(convert_one_hot(context, vocab_size))\n",
        "        else:\n",
        "            labels = np.zeros(vocab_size)\n",
        "            labels[context] = 1\n",
        "            one_hots.append(labels)\n",
        "\n",
        "    return np.array(one_hots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IFL6Hgnv68d"
      },
      "source": [
        "## CBOW\n",
        "\n",
        "The model takes a window of surrounding words as input and tries to predict the target word in the center of the window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KegZr1ZVqS5v"
      },
      "outputs": [],
      "source": [
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu9mrjxHv_N-"
      },
      "outputs": [],
      "source": [
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss(W_in)\n",
        "\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 0])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf7yJAMrpbMo"
      },
      "source": [
        "# Testing Word-embedding\n",
        "\n",
        "You can skip this chapter if you don't want to learn the dataset preprocess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3mlcz_VY9-f"
      },
      "outputs": [],
      "source": [
        "#@title Test Config\n",
        "use_pretrained = False #@param {type:\"boolean\"}\n",
        "split_policy = \"Default\" #@param [\"Japanese\", \"Chinese\", \"Default\"]\n",
        "custom_data = \" You are my friend and I am his friend. \" #@param [\"\\u6211\\u7231\\u5317\\u4EAC\\u5929\\u5B89\\u95E8\\uFF0C\\u5929\\u5B89\\u95E8\\u4E0A\\u592A\\u9633\\u5347\\u3002\\u6211\\u7231\\u5317\\u4EAC\\u6545\\u5BAB\\uFF0C\\u6545\\u5BAB\\u7684\\u592A\\u9633\\u65E9\\u5DF2\\u5347\\u8D77\\u3002\", \" You are my friend and I am his friend. \"] {allow-input: true}\n",
        "data_source = \"PTB\" #@param [\"PTB\", \"Custom\"]\n",
        "\n",
        "#@markdown ## CBOW Hyerparameters\n",
        "\n",
        "window_size = 1 #@param {type:\"integer\"}\n",
        "hidden_size = 5 #@param {type:\"integer\"}\n",
        "batch_size = 3 #@param {type:\"integer\"}\n",
        "max_epoch = 1000 #@param {type:\"slider\", min:100, max:10000, step:100}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY746REWne1s"
      },
      "source": [
        "## Test Count-based data\n",
        "\n",
        "We will use the function we created before to generate word-embedding using count-based method. If you don't need to see the data process you can skip the following code block and read the pre-trained embedding from the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvVgqEumivl",
        "outputId": "a66f182c-168c-4d85-ca00-c07ea4a15ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddding Loaded\n"
          ]
        }
      ],
      "source": [
        "if not use_pretrained:\n",
        "    corpus, word_to_id, id_to_word = load_ptb_data('train') if data_source == \"PTB\" else postprocess(custom_data, split_policy)\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "\n",
        "    C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "    print(\"Co-occurance matrix\")\n",
        "    print(C)\n",
        "\n",
        "    wordvec_size=100\n",
        "    np.set_printoptions(precision=3)\n",
        "    W = ppmi(C)\n",
        "\n",
        "    print(\"PPMIed Co-occurace Matrix\")\n",
        "    print(W)\n",
        "\n",
        "    try:\n",
        "        from sklearn.utils.extmath import randomized_svd\n",
        "        U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
        "    except ImportError:\n",
        "        U, S, V = np.linalg.svd(W)\n",
        "\n",
        "    print(\"SVDed PPMI matrix\")\n",
        "    print(U)\n",
        "\n",
        "    if len(word_to_id) < 50:\n",
        "        for word, word_id in word_to_id.items():\n",
        "            plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "\n",
        "        plt.scatter(U[:,0], U[:, 1], alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "    wordvec_size = 100\n",
        "    word_vecs = U[:, :wordvec_size]\n",
        "    count_based_embedding = (word_vecs, word_to_id, id_to_word)\n",
        "\n",
        "    with open(dataset_dir + \"/count_based_embedding.pkl\", 'wb') as f:\n",
        "        pickle.dump(count_based_embedding, f)\n",
        "\n",
        "else:\n",
        "\n",
        "    with open(dataset_dir + \"/count_based_embedding.pkl\", 'rb') as f:\n",
        "        if f:\n",
        "            print(\"Embeddding Loaded\")\n",
        "            (word_vecs, word_to_id, id_to_word) = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL6fPauYxpo2",
        "outputId": "10a58dcf-21b3-41de-b7ff-d7e0f1073d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[query] he\n",
            " she: 0.8356764912605286\n",
            " it: 0.7061509490013123\n",
            " yeargin: 0.5221908092498779\n",
            " that: 0.48959219455718994\n",
            " nobody: 0.46606236696243286\n",
            "\n",
            "[query] car\n",
            " auto: 0.6837215423583984\n",
            " truck: 0.6087413430213928\n",
            " jewelry: 0.5681161284446716\n",
            " vehicle: 0.5538378953933716\n",
            " disk-drive: 0.5514360666275024\n",
            "\n",
            "[query] bread\n",
            " peasants: 0.6837708950042725\n",
            " cubs: 0.6571336388587952\n",
            " insistence: 0.656091034412384\n",
            " toys: 0.6539305448532104\n",
            " viewpoint: 0.6513710021972656\n",
            "\n",
            "[query] watch\n",
            " ivy: 0.5914743542671204\n",
            " dignity: 0.5657815337181091\n",
            " knock: 0.5625321269035339\n",
            " reconsider: 0.5484458208084106\n",
            " send: 0.5478222370147705\n",
            "\n",
            "[query] apple\n",
            " impeachment: 0.4544225037097931\n",
            " convex: 0.44156163930892944\n",
            " printer: 0.4251265227794647\n",
            " disks: 0.41802775859832764\n",
            " chaos: 0.41405174136161804\n"
          ]
        }
      ],
      "source": [
        "querys = ['he', 'car', 'bread', 'watch', 'apple']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nmmW2X-fMU5"
      },
      "source": [
        "Let's streamline and clarify your summary:\n",
        "\n",
        "In conclusion, we've executed the following steps:\n",
        "\n",
        "1. Initially, we generated a co-occurrence matrix to represent contextual relationships between words. In this matrix, every word was unique, ensuring no duplicates.\n",
        "\n",
        "2. Following this, we transformed the co-occurrence matrix using the Positive Pointwise Mutual Information (PPMI) algorithm, thereby enhancing the representational quality.\n",
        "\n",
        "3. To mitigate the memory footprint of the matrix, we employed Singular Value Decomposition (SVD) for dimensionality reduction. This effectively eliminated null values, resulting in a denser, more compact matrix.\n",
        "\n",
        "4. Ultimately, we obtained a vector space representation for each word, facilitating efficient semantic analysis.\n",
        "\n",
        "This process is generally referred to as the **Count-based Method** for creating word embeddings.\n",
        "\n",
        "The count-based method allows us to train data only once, but each word's embedding is less meaningful than Predict-based method. Besides, as the vocabulary grows bigger, the time complexity will increase exponentially (because the matrix is n*n)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDWdeFlDo1_K"
      },
      "source": [
        "## Test Predict-based data\n",
        "\n",
        "asdfasdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate(params, grads):\n",
        "    '''\n",
        "    将参数列表中重复的权重整合为1个，\n",
        "    加上与该权重对应的梯度\n",
        "    '''\n",
        "    params, grads = params[:], grads[:]  # copy list\n",
        "\n",
        "    while True:\n",
        "        find_flg = False\n",
        "        L = len(params)\n",
        "\n",
        "        for i in range(0, L - 1):\n",
        "            for j in range(i + 1, L):\n",
        "                # 在共享权重的情况下\n",
        "                if params[i] is params[j]:\n",
        "                    grads[i] += grads[j]  # 加上梯度\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "                # 在作为转置矩阵共享权重的情况下（weight tying）\n",
        "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "\n",
        "                if find_flg: break\n",
        "            if find_flg: break\n",
        "\n",
        "        if not find_flg: break\n",
        "\n",
        "    return params, grads\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "        data_size = len(x)\n",
        "        max_iters = data_size // batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            # 打乱\n",
        "            idx = np.random.permutation(np.arange(data_size))\n",
        "            x = x[idx]\n",
        "            t = t[idx]\n",
        "\n",
        "            for iters in range(max_iters):\n",
        "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "                # 计算梯度，更新参数\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # 将共享的权重整合为1个\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # 评价\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    avg_loss = total_loss / loss_count\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
        "                    self.loss_list.append(float(avg_loss))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = np.arange(len(self.loss_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "hv4SWElMckuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5j_ZGaTp7yn"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    '''\n",
        "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "    '''\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = [], []\n",
        "            for param in params:\n",
        "                self.m.append(np.zeros_like(param))\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "\n",
        "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
        "\n",
        "if not use_pretrained:\n",
        "    corpus, word_to_id, id_to_word = load_ptb_data('train') if data_source == \"PTB\" else postprocess(custom_data, split_policy)\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "\n",
        "    contexts, target = create_contexts_target(corpus, window_size=1)\n",
        "\n",
        "    target = convert_one_hot(target, vocab_size)\n",
        "\n",
        "    contexts = convert_one_hot(contexts , vocab_size)\n",
        "\n",
        "    model = SimpleCBOW(vocab_size, hidden_size)\n",
        "    optimizer = Adam()\n",
        "    trainer = Trainer(model, optimizer)\n",
        "\n",
        "    trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "    trainer.plot()\n",
        "\n",
        "    predict_based_embedding = (model.word_vecs, word_to_id, id_to_word)\n",
        "\n",
        "\n",
        "    with open(dataset_dir + \"/predict_based_embedding.pkl\", 'wb') as f:\n",
        "        pickle.dump(predict_based_embedding, f)\n",
        "else:\n",
        "    with open(dataset_dir + \"/predict_based_embedding.pkl\", 'rb') as f:\n",
        "        if f:\n",
        "            print(\"Embeddding Loaded\")\n",
        "            (word_vecs, word_to_id, id_to_word) = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWbv_HsatDUh"
      },
      "source": [
        "# Data Process\n",
        "\n",
        "sdaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdkeSF1PtgJo"
      },
      "outputs": [],
      "source": [
        "#@title Text Process Config\n",
        "embedding_method = \"word2vec\" #@param [\"word2vec\", \"Count-based\"]\n",
        "use_cached_embedding = True #@param {type:\"boolean\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7oBm2MEtFSu"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "We should clear that the real-world data is not as clean as we assumed. Thet usually:\n",
        "\n",
        "* Differs by length\n",
        "* Has some non-character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTEd8KSotcz0",
        "outputId": "ae02bf25-832a-46f3-8bd5-66f3b554ba4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.75908351  0.75383226  0.86338345 -0.88820116  1.05683732]\n"
          ]
        }
      ],
      "source": [
        "if use_cached_embedding:\n",
        "    if embedding_method == \"Count-based\":\n",
        "        with open(dataset_dir + \"count_based_embedding.pkl\", 'rb') as f:\n",
        "            if f: (word_vecs, word_to_id, id_to_word) = pickle.load(f)\n",
        "    elif embedding_method == \"word2vec\":\n",
        "        with open(dataset_dir + \"/predict_based_embedding.pkl\", 'rb') as f:\n",
        "            if f: (word_vecs, word_to_id, id_to_word) = pickle.load(f)\n",
        "\n",
        "\n",
        "def sentence_to_vector(sentence, model=None):\n",
        "    # Note the shape[0] represnt the dataset's total word count. Because the matrix is actually a n*n shape\n",
        "    sentence_vector = np.zeros(word_vecs.shape[1])\n",
        "\n",
        "    num_words = 0\n",
        "\n",
        "    for word in sentence.lower().split(\" \"):\n",
        "        if word in word_to_id:\n",
        "            sentence_vector += word_vecs[word_to_id[word]]\n",
        "\n",
        "        num_words += 1\n",
        "\n",
        "    # If the sentence is not empty, divide the sum by the number of words to get the average\n",
        "    if num_words > 0:\n",
        "        sentence_vector /= num_words\n",
        "\n",
        "    return sentence_vector\n",
        "\n",
        "print(sentence_to_vector(\"Friend New Friend\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6chCC5d4zcf6"
      },
      "source": [
        "# Trainer: Spam Message Filter\n",
        "\n",
        "If you want to know how ne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEHxCoiNw_fu"
      },
      "outputs": [],
      "source": [
        "#@title Trainer Config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgfsoF34sYe0"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset is mainly from \"\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdPU87VpZWdG"
      },
      "outputs": [],
      "source": [
        "def read_dataset(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    labels = []\n",
        "    vectors = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.strip().split(None, 1)\n",
        "\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        label, message = parts\n",
        "\n",
        "        # Convert the label to binary (1 for spam, 0 for ham)\n",
        "        label = 1 if label == 'spam' else 0\n",
        "\n",
        "        # Convert the message to a vector using the sentence_to_vector function\n",
        "        vector = sentence_to_vector(message)\n",
        "\n",
        "        # Add the label and vector to the lists\n",
        "        labels.append(label)\n",
        "        vectors.append(vector)\n",
        "\n",
        "    return labels, vectors\n",
        "\n",
        "labels, vectors = read_dataset('/content/SMSSpamCollection.txt')\n",
        "\n",
        "labels = np.array(labels)\n",
        "embedded_messages = np.array(vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lHMj9lssg-t"
      },
      "source": [
        "## Basic Classfier\n",
        "\n",
        "To learn more about each layer you can read this [notebook](https://colab.research.google.com/drive/18B-Fujnr7uDhfyERZzWHTI3-31anw5OH?usp=sharing) first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOJIOF6vtuHn",
        "outputId": "03645045-2ea3-4004-85a0-84e0badc7daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "141/141 [==============================] - 2s 5ms/step - loss: 0.5905 - accuracy: 0.8560 - val_loss: 0.4089 - val_accuracy: 0.8737\n",
            "Epoch 2/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.4168 - accuracy: 0.8595 - val_loss: 0.3519 - val_accuracy: 0.8737\n",
            "Epoch 3/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3723 - accuracy: 0.8595 - val_loss: 0.3428 - val_accuracy: 0.8737\n",
            "Epoch 4/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3477 - accuracy: 0.8595 - val_loss: 0.3200 - val_accuracy: 0.8737\n",
            "Epoch 5/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3423 - accuracy: 0.8595 - val_loss: 0.3047 - val_accuracy: 0.8737\n",
            "Epoch 6/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3330 - accuracy: 0.8595 - val_loss: 0.3019 - val_accuracy: 0.8737\n",
            "Epoch 7/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3246 - accuracy: 0.8773 - val_loss: 0.2913 - val_accuracy: 0.9199\n",
            "Epoch 8/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3206 - accuracy: 0.8885 - val_loss: 0.2833 - val_accuracy: 0.9164\n",
            "Epoch 9/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3155 - accuracy: 0.8927 - val_loss: 0.2818 - val_accuracy: 0.9164\n",
            "Epoch 10/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.3054 - accuracy: 0.8980 - val_loss: 0.2773 - val_accuracy: 0.9199\n",
            "Epoch 11/20\n",
            "141/141 [==============================] - 1s 5ms/step - loss: 0.3048 - accuracy: 0.9074 - val_loss: 0.2709 - val_accuracy: 0.9173\n",
            "Epoch 12/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.3012 - accuracy: 0.9092 - val_loss: 0.2800 - val_accuracy: 0.8986\n",
            "Epoch 13/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.3063 - accuracy: 0.9038 - val_loss: 0.2843 - val_accuracy: 0.8906\n",
            "Epoch 14/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.2972 - accuracy: 0.9107 - val_loss: 0.2820 - val_accuracy: 0.9404\n",
            "Epoch 15/20\n",
            "141/141 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.9159 - val_loss: 0.2631 - val_accuracy: 0.9368\n",
            "Epoch 16/20\n",
            "141/141 [==============================] - 1s 6ms/step - loss: 0.2897 - accuracy: 0.9230 - val_loss: 0.2576 - val_accuracy: 0.9235\n",
            "Epoch 17/20\n",
            "141/141 [==============================] - 1s 5ms/step - loss: 0.2896 - accuracy: 0.9201 - val_loss: 0.2558 - val_accuracy: 0.9217\n",
            "Epoch 18/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.2896 - accuracy: 0.9176 - val_loss: 0.2579 - val_accuracy: 0.9244\n",
            "Epoch 19/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.2873 - accuracy: 0.9207 - val_loss: 0.2589 - val_accuracy: 0.9173\n",
            "Epoch 20/20\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.2910 - accuracy: 0.9219 - val_loss: 0.2497 - val_accuracy: 0.9466\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.9466\n",
            "Loss: 0.2496841549873352, Accuracy: \u001b[34m0.9466192126274109\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embedded_messages, labels, test_size=0.2)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)), # assuming input vectors are 1D\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # output layer for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Loss: {loss}, Accuracy: {Fore.BLUE}{accuracy}{Fore.RESET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72nt553Cz4ps",
        "outputId": "8ce6e149-6901-4d98-f779-2e3008308725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mSPAM  \u001b[39m\u001b[33mPlease call our customer service to get a free coupon\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mWhat's your plan of this Saturaday?\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mI love you Cathy\u001b[39m\n",
            "\u001b[31mSPAM  \u001b[39m\u001b[33mFor only 6 Rewards points, scroll through your social feed worry-free with 500MB data, valid for 1 day!\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33m[Leetcode] 42094 is You login code. Do not share it with anyone.\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mLong time no see my old friend.\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mMake everyone green with envy with our new collection. Shop now at www.fakeshoppingsite.com\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mYour OKX verification code is: 443287. This code will expire in 10 minutes. Don't share this code with anyone; our employees will never ask for the code.\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mHurry up! Limited time offer at www.fakeofferwebsite.com\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "def predict_spam(model, sentence):\n",
        "    vector = sentence_to_vector(sentence)\n",
        "    vector = np.expand_dims(vector, axis=0)\n",
        "    prediction = model.predict(vector, verbose=0)\n",
        "    label = 1 if prediction > 0.5 else 0\n",
        "    return label\n",
        "\n",
        "sentences = [\n",
        "    \"Please call our customer service to get a free coupon\",\n",
        "    \"What's your plan of this Saturaday?\",\n",
        "    \"I love you Cathy\",\n",
        "    \"For only 6 Rewards points, scroll through your social feed worry-free with 500MB data, valid for 1 day!\",\n",
        "    \"[Leetcode] 42094 is You login code. Do not share it with anyone.\",\n",
        "    \"Long time no see my old friend.\",\n",
        "    \"Make everyone green with envy with our new collection. Shop now at www.fakeshoppingsite.com\",\n",
        "    \"Your OKX verification code is: 443287. This code will expire in 10 minutes. Don't share this code with anyone; our employees will never ask for the code.\",\n",
        "    \"Hurry up! Limited time offer at www.fakeofferwebsite.com\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    prediction = predict_spam(model, s)\n",
        "    print(f\"{Fore.RED + 'SPAM  ' + Fore.RESET if prediction == 1 else  Fore.GREEN +  'HAM   ' + Fore.RESET}{Fore.YELLOW + s + Fore.RESET}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLT8ZQLLv5Re"
      },
      "source": [
        "# Trainer: Text Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XwkDgJRNlIw"
      },
      "source": [
        "# Reference\n",
        "\n",
        "1. Saito Kokih. Deep Learning from Scratch 3: Natural Language Processing[M]. Japan: O'Reilly Japan, 2018.\n",
        "\n",
        "2. Khelifi Ahmed Aziz. Medium. Learn How to Write Markdown & LaTeX in The Jupyter Notebook (https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)\n",
        "\n",
        "3. lukesalamone. What is Temperature (https://lukesalamone.github.io/posts/what-is-temperature/)\n",
        "\n",
        "4. jalammar. The Illustrated Word2Vec ([https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/))\n",
        "\n",
        "5. jalammar. The Illustrated Transformer ([http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "12ylRxbXYiuZqODUKW3yrBHGwXU9MLp0s",
      "authorship_tag": "ABX9TyOWHZuv5OiaPo1gnCG+Zzom",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}