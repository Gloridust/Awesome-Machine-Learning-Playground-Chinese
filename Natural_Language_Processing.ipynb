{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12ylRxbXYiuZqODUKW3yrBHGwXU9MLp0s",
      "authorship_tag": "ABX9TyND4ZxbHym+AXE8iWz0PEJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverTwilight/Awesome-Machine-Learning-Playground/blob/master/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "> By Rene Wang\n",
        "\n",
        "This notebook provides a comprehensive explanation of how a basic language model operates, covering all the essential steps. Upon completion, you will possess a solid foundational understanding of Natural Language Processing (NLP), enabling you to delve deeper into the subject.\n",
        "\n",
        "Note: It's recommend to read the [Handwritten_Digits_Detection]() first. Some basic concepts and codes are explained there."
      ],
      "metadata": {
        "id": "mDTIwtCYALv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import jieba\n",
        "import pickle\n",
        "!pip install kora colorama -q\n",
        "from colorama import Fore, Back, Style\n",
        "from kora import drive\n",
        "drive.link_nbs()\n",
        "\n",
        "# from Handwritten_Digits_Detection import Relu, Affine, AdaGard"
      ],
      "metadata": {
        "id": "_-XZlvzavc55"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To render Chinese character in matplotlib we need to download the font first."
      ],
      "metadata": {
        "id": "5iL0SC9k2rdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O SourceHanSansSC-Normal.otf https://github.com/adobe-fonts/source-han-sans/blob/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf?raw=true\n",
        "\n",
        "os.makedirs('/root/.config/matplotlib', exist_ok=True)\n",
        "\n",
        "!cp SourceHanSansSC-Normal.otf /root/.config/matplotlib/\n",
        "\n",
        "matplotlib.font_manager.fontManager.addfont('SourceHanSansSC-Normal.otf')\n",
        "\n",
        "matplotlib.rc(\"font\",family='Source Han Sans SC')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ix4iu8ucmch",
        "outputId": "7f085e41-3c35-4e3e-f929-ab1eeb59ab93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-09 10:06:38--  https://github.com/adobe-fonts/source-han-sans/blob/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf?raw=true\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf [following]\n",
            "--2023-06-09 10:06:38--  https://github.com/adobe-fonts/source-han-sans/raw/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/adobe-fonts/source-han-sans/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf [following]\n",
            "--2023-06-09 10:06:39--  https://raw.githubusercontent.com/adobe-fonts/source-han-sans/release/OTF/SimplifiedChinese/SourceHanSansSC-Normal.otf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16414944 (16M) [application/octet-stream]\n",
            "Saving to: ‘SourceHanSansSC-Normal.otf’\n",
            "\n",
            "SourceHanSansSC-Nor 100%[===================>]  15.65M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-06-09 10:06:39 (178 MB/s) - ‘SourceHanSansSC-Normal.otf’ saved [16414944/16414944]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding: Count-based Method\n",
        "\n",
        "In order to put the complex the data into neural network, the most-used solution is to convert the data to vector space, which is called **word2vec**. In this part we'll discuss how to convert the natural text data into vectors."
      ],
      "metadata": {
        "id": "VXyinmCk9vVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word co-occurrence matrix\n",
        "\n",
        "Usually the meaning of a word depends on the context. A word co-occurrence matrix reprents how many times has a word appear around another word. \n",
        "\n",
        "Consider this sentence:\n",
        "```markdown\n",
        " You are my friend and I am his friend. \n",
        "```\n",
        "First we remove the duplicate words and we can create a table like this:\n",
        "\n",
        "\\begin{array}{ccc}\n",
        "\\text{ }&\\text{You}&\\text{are}&\\text{my}&\\text{friend}&\\text{and}&\\text{I}&\\text{am}&\\text{his}&\\text{.}\\\\\n",
        "\\text{You}&\\text{0}&\\text{1}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{are}&\\text{1}&\\text{0}&\\text{1}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}&\\text{0}\\\\\n",
        "\\text{my}\\\\\n",
        "\\text{friend}\\\\\n",
        "\\text{and}\\\\\n",
        "\\text{I}\\\\\n",
        "\\text{am}\\\\\n",
        "\\text{his}\\\\\n",
        "\\end{array}\n",
        "\n",
        "The row label reprents those words exsit in the context of the col lable. The context size was called **window_size**."
      ],
      "metadata": {
        "id": "u_W945-M3fon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('Use Python3!')\n",
        "import pickle\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Project/NLP'\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = dataset_dir + '/' + vocab_file\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def load_ptb_data(data_type='train'):\n",
        "    '''\n",
        "        :param data_type: 数据的种类：'train' or 'test' or 'valid (val)'\n",
        "        :return:\n",
        "    '''\n",
        "    if data_type == 'val': data_type = 'valid'\n",
        "    save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "    word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        corpus = np.load(save_path)\n",
        "        return corpus, word_to_id, id_to_word\n",
        "\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    np.save(save_path, corpus)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "def postprocess(text, split_policy=\"Default\"):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', \" .\")\n",
        "\n",
        "    if split_policy == \"Chinese\":\n",
        "        words = list(jieba.cut(text))\n",
        "    else:\n",
        "        words = text.split(' ')\n",
        "\n",
        "    print(words)\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "\n",
        "    for word in words:\n",
        "        new_id = len(word_to_id)\n",
        "        word_to_id.setdefault(word, new_id)\n",
        "        id_to_word.setdefault(new_id, word)\n",
        "    \n",
        "    corpus = [word_to_id[w] for w in words]\n",
        "    corpus = np.array(corpus)\n",
        "\n",
        "    return corpus, word_to_id, id_to_word"
      ],
      "metadata": {
        "id": "LSGRJnCBmsae"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_co_matrix(corpus, vocab_size, windows_size=1):\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(windows_size):\n",
        "            left_idx = idx - 1\n",
        "            right_idx = idx + 1\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix"
      ],
      "metadata": {
        "id": "SU7QRZBxps3S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPMI & SVD\n",
        "\n",
        "But this doesn't represnet the link between word very well. Thus we introduced the PMI. The $P(x)$ denote the probablity of the event x.\n",
        "\n",
        "$$ PMI(x, y) = log_{2}\\frac{P(x, y)}{P(x)P(y)} $$\n",
        "\n",
        "But as the volcabulary grows, use a 2-dimension array like this is not efficient. So we introduced the Singular Value Decomposition, or SVD, to reduce the dimension.\n",
        "\n",
        "$$ X=USV^{t} $$\n",
        "\n",
        "In the graph, the nearer the two word are, it means the similerer the two words are. Following cell will implement PPMI & SVD to the matrix."
      ],
      "metadata": {
        "id": "FImNwpIcry0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppmi(C, verbose=False, eps=1e-8):\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100+1) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "\n",
        "    return M"
      ],
      "metadata": {
        "id": "APd_lfTcr9pf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Comparsion\n",
        "\n",
        "Our goal is to make the AI know the word based on their meaning. So we have to calculate the similarity between the words.\n",
        "\n",
        "For example, we want the machine know that \"I\", \"You\" and \"He\" are simliary words in the English. This part we will **calculate the similarity of vectors**.\n",
        "\n",
        "We have servals alogorithum to finish the tasks:\n",
        "\n",
        "* Euclidean Distance\n",
        "* Cosine Similarity\n",
        "* Manhattan Distance"
      ],
      "metadata": {
        "id": "RnEOqnlj20k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
        "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    if query not in word_to_id:\n",
        "        print('%s is not found' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return"
      ],
      "metadata": {
        "id": "KzCooXjkq2By"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-embedding: Predict-based Method\n",
        "\n",
        "By using this method, we can get a more-connected word-emnbedding. This method is also called word2vec, which foucus on predict the word with given context or vise versa."
      ],
      "metadata": {
        "id": "QHMPpGuZvBym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Word-embedding Data\n",
        "\n",
        "You can skip this chapter if you don't want to learn the dataset preprocess."
      ],
      "metadata": {
        "id": "rf7yJAMrpbMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Config\n",
        "use_pretrained = True #@param {type:\"boolean\"}\n",
        "split_policy = \"Default\" #@param [\"Japanese\", \"Chinese\", \"Default\"]\n",
        "custom_data = \"\\u6211\\u7231\\u5317\\u4EAC\\u5929\\u5B89\\u95E8\\uFF0C\\u5929\\u5B89\\u95E8\\u4E0A\\u592A\\u9633\\u5347\\u3002\\u6211\\u7231\\u5317\\u4EAC\\u6545\\u5BAB\\uFF0C\\u6545\\u5BAB\\u7684\\u592A\\u9633\\u65E9\\u5DF2\\u5347\\u8D77\\u3002\" #@param [\"\\u6211\\u7231\\u5317\\u4EAC\\u5929\\u5B89\\u95E8\\uFF0C\\u5929\\u5B89\\u95E8\\u4E0A\\u592A\\u9633\\u5347\\u3002\\u6211\\u7231\\u5317\\u4EAC\\u6545\\u5BAB\\uFF0C\\u6545\\u5BAB\\u7684\\u592A\\u9633\\u65E9\\u5DF2\\u5347\\u8D77\\u3002\", \" You are my friend and I am his friend. \"] {allow-input: true}\n",
        "data_source = \"PTB\" #@param [\"PTB\", \"Custom\"]\n"
      ],
      "metadata": {
        "id": "K3mlcz_VY9-f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Count-based data\n",
        "\n",
        "We will use the function we created before to generate word-embedding using count-based method. If you don't need to see the data process you can skip the following code block and read the pre-trained embedding from the drive."
      ],
      "metadata": {
        "id": "NY746REWne1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not use_pretrained:\n",
        "    corpus, word_to_id, id_to_word = load_ptb_data('train') if data_source == \"PTB\" else postprocess(custom_data, split_policy)\n",
        "\n",
        "    vocab_size = len(word_to_id)\n",
        "\n",
        "    C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "    print(\"Co-occurance matrix\")\n",
        "    print(C)\n",
        "\n",
        "    wordvec_size=100\n",
        "    np.set_printoptions(precision=3)\n",
        "    W = ppmi(C)\n",
        "\n",
        "    print(\"PPMIed Co-occurace Matrix\")\n",
        "    print(W)\n",
        "\n",
        "    try:\n",
        "        from sklearn.utils.extmath import randomized_svd\n",
        "        U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
        "    except ImportError:\n",
        "        U, S, V = np.linalg.svd(W)\n",
        "\n",
        "    print(\"SVDed PPMI matrix\")\n",
        "    print(U)\n",
        "\n",
        "    if len(word_to_id) < 50:\n",
        "        for word, word_id in word_to_id.items():\n",
        "            plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "\n",
        "        plt.scatter(U[:,0], U[:, 1], alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "    wordvec_size = 100\n",
        "    word_vecs = U[:, :wordvec_size]\n",
        "    count_based_embedding = (word_vecs, word_to_id, id_to_word)\n",
        "\n",
        "    with open(dataset_dir + \"/count_based_embedding.pkl\", 'wb') as f:\n",
        "        pickle.dump(count_based_embedding, f)\n",
        "\n",
        "else:\n",
        "\n",
        "    with open(dataset_dir + \"/count_based_embedding.pkl\", 'rb') as f:\n",
        "        if f:\n",
        "            print(\"Embeddding Loaded\")\n",
        "            (word_vecs, word_to_id, id_to_word) = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvVgqEumivl",
        "outputId": "ebfbd903-9f83-4f0f-e38f-e4b0ff465fb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddding Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "querys = ['he', 'car', 'bread', 'watch', 'way']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "id": "tL6fPauYxpo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e9211-4c34-4aaf-b5f1-720050640f26"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] he\n",
            " she: 0.8356764912605286\n",
            " it: 0.7061509490013123\n",
            " yeargin: 0.5221908092498779\n",
            " that: 0.48959219455718994\n",
            " nobody: 0.46606236696243286\n",
            "\n",
            "[query] car\n",
            " auto: 0.6837215423583984\n",
            " truck: 0.6087413430213928\n",
            " jewelry: 0.5681161284446716\n",
            " vehicle: 0.5538378953933716\n",
            " disk-drive: 0.5514360666275024\n",
            "\n",
            "[query] bread\n",
            " peasants: 0.6837708950042725\n",
            " cubs: 0.6571336388587952\n",
            " insistence: 0.656091034412384\n",
            " toys: 0.6539305448532104\n",
            " viewpoint: 0.6513710021972656\n",
            "\n",
            "[query] watch\n",
            " ivy: 0.5914743542671204\n",
            " dignity: 0.5657815337181091\n",
            " knock: 0.5625321269035339\n",
            " reconsider: 0.5484458208084106\n",
            " send: 0.5478222370147705\n",
            "\n",
            "[query] way\n",
            " sign: 0.539264976978302\n",
            " knowledge: 0.5219477415084839\n",
            " chance: 0.4759937822818756\n",
            " dignity: 0.47081226110458374\n",
            " getting: 0.447970449924469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's streamline and clarify your summary:\n",
        "\n",
        "In conclusion, we've executed the following steps:\n",
        "\n",
        "1. Initially, we generated a co-occurrence matrix to represent contextual relationships between words. In this matrix, every word was unique, ensuring no duplicates.\n",
        "\n",
        "2. Following this, we transformed the co-occurrence matrix using the Positive Pointwise Mutual Information (PPMI) algorithm, thereby enhancing the representational quality.\n",
        "\n",
        "3. To mitigate the memory footprint of the matrix, we employed Singular Value Decomposition (SVD) for dimensionality reduction. This effectively eliminated null values, resulting in a denser, more compact matrix.\n",
        "\n",
        "4. Ultimately, we obtained a vector space representation for each word, facilitating efficient semantic analysis.\n",
        "\n",
        "This process is generally referred to as the **Count-based Method** for creating word embeddings.\n",
        "\n",
        "The count-based method allows us to train data only once, but each word's embedding is less meaningful than Predict-based method. Besides, as the vocabulary grows bigger, the time complexity will increase exponentially (because the matrix is n*n)."
      ],
      "metadata": {
        "id": "4nmmW2X-fMU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Predict-based data\n",
        "\n",
        "asdfasdf"
      ],
      "metadata": {
        "id": "zDWdeFlDo1_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "X5j_ZGaTp7yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Process\n",
        "\n",
        "sdaf"
      ],
      "metadata": {
        "id": "AWbv_HsatDUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text Process Config\n",
        "embedding_method = \"Count-based\" #@param [\"word2vec\", \"Count-based\"]\n",
        "use_cached_embedding = False #@param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "id": "EdkeSF1PtgJo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "\n",
        "We should clear that the real-world data is not as clean as we assumed. Thet usually:\n",
        "\n",
        "* Differs by length\n",
        "* Has some non-character"
      ],
      "metadata": {
        "id": "z7oBm2MEtFSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_cached_embedding:\n",
        "    if embedding_method == \"Count-based\":\n",
        "        with open(dataset_dir + \"count_based_embedding.pkl\", 'rb') as f:\n",
        "            if f: (word_vecs, word_to_id, id_to_word) = pickle.load(f)\n",
        "    elif embedding_method == \"Count-based\":\n",
        "        with open(dataset_dir + \"predict_based_embedding.pkl\", 'rb') as f:\n",
        "            if f: (word_vecs, word_to_id, id_to_word) = pickle.load(f)\n",
        "\n",
        "def sentence_to_vector(sentence, model=None):\n",
        "    # Note the shape[0] represnt the dataset's total word count. Because the matrix is actually a n*n shape\n",
        "    sentence_vector = np.zeros(word_vecs.shape[1])\n",
        "\n",
        "    num_words = 0\n",
        "    \n",
        "    for word in sentence.lower().split(\" \"):\n",
        "        if embedding_method == \"Count-based\":\n",
        "            if word in word_to_id:\n",
        "                sentence_vector += word_vecs[word_to_id[word]]\n",
        "        else:\n",
        "            sentence_vector += model.wv[word]\n",
        "                \n",
        "        num_words += 1\n",
        "    \n",
        "    # If the sentence is not empty, divide the sum by the number of words to get the average\n",
        "    if num_words > 0:\n",
        "        sentence_vector /= num_words\n",
        "    \n",
        "    return sentence_vector\n",
        "\n",
        "print(sentence_to_vector(\"The New Jersey Devils and the Detroit Red Wings play Ice Hockey.\"))"
      ],
      "metadata": {
        "id": "CTEd8KSotcz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e7e022-4497-4632-b67f-e48c015e54cb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.578e-02  2.828e-02  9.418e-03 -1.313e-02  4.411e-03 -8.618e-03\n",
            "  2.677e-03  1.972e-02  7.540e-03 -8.971e-03  3.138e-02  6.585e-03\n",
            " -2.964e-03 -2.060e-02  7.670e-03 -7.219e-03 -1.078e-02  3.520e-03\n",
            " -5.052e-03  1.339e-02  4.126e-03 -1.232e-03 -2.193e-02 -3.325e-03\n",
            "  1.934e-02  3.677e-03 -7.852e-03  2.615e-03 -8.546e-03 -7.703e-03\n",
            " -7.261e-03 -8.996e-03  1.437e-02 -9.756e-03 -1.098e-03 -8.365e-03\n",
            "  5.561e-03 -1.681e-02 -8.022e-03 -1.235e-02  6.034e-03 -1.096e-02\n",
            "  5.051e-03 -4.538e-03  4.897e-03  1.592e-02  5.728e-03 -5.001e-03\n",
            " -1.843e-02 -6.627e-03 -3.112e-03 -3.591e-03 -1.885e-03 -9.115e-03\n",
            "  3.566e-03 -3.167e-03 -2.176e-03 -8.351e-03  1.270e-03 -6.808e-03\n",
            " -1.799e-02 -8.503e-03 -2.359e-03 -8.527e-03  2.015e-03 -8.298e-03\n",
            " -7.903e-03  1.642e-02  3.235e-03 -2.093e-03  3.239e-04  4.637e-03\n",
            " -5.446e-04  7.157e-03 -5.292e-03  1.335e-03 -8.376e-03  1.175e-03\n",
            "  4.807e-03 -6.613e-03 -1.924e-05  1.102e-02 -1.196e-02 -4.392e-03\n",
            "  9.361e-03 -3.458e-03 -6.001e-03  1.307e-02  2.744e-03 -1.024e-02\n",
            "  1.027e-02  8.414e-05 -5.326e-03 -2.998e-03  2.209e-03  5.472e-04\n",
            " -3.075e-03 -3.436e-04 -8.132e-03 -1.129e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer: Spam Message Filter\n",
        "\n",
        "If you want to know how ne"
      ],
      "metadata": {
        "id": "6chCC5d4zcf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trainer Config\n"
      ],
      "metadata": {
        "id": "xEHxCoiNw_fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "MgfsoF34sYe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Initialize empty lists to hold the labels and message vectors\n",
        "    labels = []\n",
        "    vectors = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Split the line into label and message at the first whitespace character\n",
        "        parts = line.strip().split(None, 1)\n",
        "\n",
        "        # If the line doesn't have at least two parts, skip it\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        label, message = parts\n",
        "\n",
        "        # Convert the label to binary (1 for spam, 0 for ham)\n",
        "        label = 1 if label == 'spam' else 0\n",
        "\n",
        "        # Convert the message to a vector using the sentence_to_vector function\n",
        "        vector = sentence_to_vector(message)\n",
        "\n",
        "        # Add the label and vector to the lists\n",
        "        labels.append(label)\n",
        "        vectors.append(vector)\n",
        "\n",
        "    return labels, vectors\n",
        "\n",
        "labels, vectors = read_dataset('/content/SMSSpamCollection.txt')\n",
        "\n",
        "# Optionally, you may want to convert the lists to numpy arrays for use with many machine learning libraries\n",
        "labels = np.array(labels)\n",
        "embedded_messages = np.array(vectors)"
      ],
      "metadata": {
        "id": "wdPU87VpZWdG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Classfier"
      ],
      "metadata": {
        "id": "-lHMj9lssg-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'embedded_messages' is your list of message vectors and 'labels' is your list of labels\n",
        "\n",
        "# Split the dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(embedded_messages, labels, test_size=0.2)\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)), # assuming input vectors are 1D\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # output layer for binary classification\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOJIOF6vtuHn",
        "outputId": "f16e81a0-6d9b-4996-ffb7-922ee4460e30"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 1s 3ms/step - loss: 0.6097 - accuracy: 0.8524 - val_loss: 0.3899 - val_accuracy: 0.8879\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.8560 - val_loss: 0.3420 - val_accuracy: 0.8879\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8560 - val_loss: 0.3221 - val_accuracy: 0.8879\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3549 - accuracy: 0.8560 - val_loss: 0.3119 - val_accuracy: 0.8879\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.8560 - val_loss: 0.3030 - val_accuracy: 0.8879\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8560 - val_loss: 0.2849 - val_accuracy: 0.8879\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8673 - val_loss: 0.2862 - val_accuracy: 0.9128\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8887 - val_loss: 0.2751 - val_accuracy: 0.9119\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8902 - val_loss: 0.2694 - val_accuracy: 0.9101\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8996 - val_loss: 0.2773 - val_accuracy: 0.9199\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 0.2773 - accuracy: 0.9199\n",
            "Loss: 0.2773182690143585, Accuracy: 0.9199288487434387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_spam(model, sentence):\n",
        "    # Convert the sentence to vector\n",
        "    vector = sentence_to_vector(sentence)\n",
        "\n",
        "    # Remember to match the shape of the input your model expects. \n",
        "    # If your model was trained on single samples of shape (N,), you may need to expand the dimensions of your input\n",
        "    vector = np.expand_dims(vector, axis=0)\n",
        "\n",
        "    # Use the model to predict the probability of the sentence being spam\n",
        "    prediction = model.predict(vector, verbose=0)\n",
        "\n",
        "    # Since we use a sigmoid activation function in our final layer, \n",
        "    # the output will be a number between 0 and 1 representing the probability that the sentence is spam.\n",
        "    # We can convert this to a binary label by choosing a threshold (like 0.5) and classifying all sentences \n",
        "    # with a probability greater than the threshold as spam (1) and all others as not spam (0).\n",
        "    label = 1 if prediction > 0.5 else 0\n",
        "\n",
        "    return label\n",
        "\n",
        "sentences = [\n",
        "    \"Please call our customer service to get a free coupon\",\n",
        "    \"What's your plan of this Saturaday?\",\n",
        "    \"I love you Cathy\",\n",
        "    \"For only 6 Rewards points, scroll through your social feed worry-free with 500MB data, valid for 1 day!\",\n",
        "    \"[Leetcode] 42094 is You login code. Do not share it with anyone.\",\n",
        "    \"Long time no see my old friend.\",\n",
        "    \"Make everyone green with envy with our new collection. Shop now at www.fakeshoppingsite.com\",\n",
        "    \"Your OKX verification code is: 443287. This code will expire in 10 minutes. Don't share this code with anyone; our employees will never ask for the code.\",\n",
        "    \"Hurry up! Limited time offer at www.fakeofferwebsite.com\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    prediction = predict_spam(model, s)\n",
        "    print(f\"{Fore.RED + 'SPAM  ' + Fore.RESET if prediction == 1 else  Fore.GREEN +  'HAM   ' + Fore.RESET}{Fore.YELLOW + s + Fore.RESET}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72nt553Cz4ps",
        "outputId": "11ab9afa-9c02-4495-971c-6ca82bd6c24d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mSPAM  \u001b[39m\u001b[33mPlease call our customer service to get a free coupon\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mWhat's your plan of this Saturaday?\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mI love you Cathy\u001b[39m\n",
            "\u001b[31mSPAM  \u001b[39m\u001b[33mFor only 6 Rewards points, scroll through your social feed worry-free with 500MB data, valid for 1 day!\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33m[Leetcode] 42094 is You login code. Do not share it with anyone.\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mLong time no see my old friend.\u001b[39m\n",
            "\u001b[31mSPAM  \u001b[39m\u001b[33mMake everyone green with envy with our new collection. Shop now at www.fakeshoppingsite.com\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mYour OKX verification code is: 443287. This code will expire in 10 minutes. Don't share this code with anyone; our employees will never ask for the code.\u001b[39m\n",
            "\u001b[32mHAM   \u001b[39m\u001b[33mHurry up! Limited time offer at www.fakeofferwebsite.com\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optmized Classfier"
      ],
      "metadata": {
        "id": "8Idn60sJvsNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer: Translator"
      ],
      "metadata": {
        "id": "vLT8ZQLLv5Re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "1. Saito Kokih. Deep Learning from Scratch 4: Natural Language Processing[M]. Japan: O'Reilly Japan, 2018.\n",
        "\n",
        "2. Khelifi Ahmed Aziz. Medium. Learn How to Write Markdown & LaTeX in The Jupyter Notebook (https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)"
      ],
      "metadata": {
        "id": "1XwkDgJRNlIw"
      }
    }
  ]
}