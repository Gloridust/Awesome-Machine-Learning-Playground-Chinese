{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOT+r6Ko6UDDrSy1j1vTK1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverTwilight/Awesome-Machine-Learning-Playground/blob/master/Pixel_Spirite_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "We will create a model"
      ],
      "metadata": {
        "id": "7GxLKzMZkEDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "\n",
        "!git clone https://github.com/RiverTwilight/Awesome-Machine-Learning-Playground.git\n",
        "!mv ./Awesome-Machine-Learning-Playground ./project\n",
        "\n",
        "from project.diffusion_utilities import CustomDataset, transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiZNX9WfkHwM",
        "outputId": "adb148c7-bb7d-486e-a1e5-c7e7f280ab99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Awesome-Machine-Learning-Playground'...\n",
            "remote: Enumerating objects: 995, done.\u001b[K\n",
            "remote: Counting objects: 100% (995/995), done.\u001b[K\n",
            "remote: Compressing objects: 100% (972/972), done.\u001b[K\n",
            "remote: Total 995 (delta 35), reused 969 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (995/995), 21.50 MiB | 16.53 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "mv: cannot move './Awesome-Machine-Learning-Playground' to './project/Awesome-Machine-Learning-Playground': Directory not empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyper Paramters\n",
        "batch_size = 100 #@param {type:\"integer\"}\n",
        "n_epoch = 32 #@param {type:\"integer\"}\n",
        "lrate=1e-3 #@param {type:\"number\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "6zagO6pMuOkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocess\n",
        "\n",
        "We need to add nosie to the image."
      ],
      "metadata": {
        "id": "3qm3qCyil1kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset and construct optimizer\n",
        "dataset = CustomDataset(\"./project/dataset/spirite/sprites_1788_16x16.npy\", \"./project/dataset/spirite/sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmiPZXg1qEop",
        "outputId": "b19decb4-6f0c-4b47-d157-bb5cf5129fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sprite shape: (89400, 16, 16, 3)\n",
            "labels shape: (89400, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "asfd"
      ],
      "metadata": {
        "id": "By-e6mZbvI8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional layer\n",
        "\n",
        "asdf"
      ],
      "metadata": {
        "id": "bJ6EC1Z4vSUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Check if input and output channels are the same for the residual connection\n",
        "        self.same_channels = in_channels == out_channels\n",
        "\n",
        "        # Flag for whether or not to use residual connection\n",
        "        self.is_res = is_res\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n",
        "            nn.BatchNorm2d(out_channels),   # Batch normalization\n",
        "            nn.GELU(),   # GELU activation function\n",
        "        )\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n",
        "            nn.BatchNorm2d(out_channels),   # Batch normalization\n",
        "            nn.GELU(),   # GELU activation function\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # If using residual connection\n",
        "        if self.is_res:\n",
        "            # Apply first convolutional layer\n",
        "            x1 = self.conv1(x)\n",
        "\n",
        "            # Apply second convolutional layer\n",
        "            x2 = self.conv2(x1)\n",
        "\n",
        "            # If input and output channels are the same, add residual connection directly\n",
        "            if self.same_channels:\n",
        "                out = x + x2\n",
        "            else:\n",
        "                # If not, apply a 1x1 convolutional layer to match dimensions before adding residual connection\n",
        "                shortcut = nn.Conv2d(x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0).to(x.device)\n",
        "                out = shortcut(x) + x2\n",
        "            #print(f\"resconv forward: x {x.shape}, x1 {x1.shape}, x2 {x2.shape}, out {out.shape}\")\n",
        "\n",
        "            # Normalize output tensor\n",
        "            return out / 1.414\n",
        "\n",
        "        # If not using residual connection, return output of second convolutional layer\n",
        "        else:\n",
        "            x1 = self.conv1(x)\n",
        "            x2 = self.conv2(x1)\n",
        "            return x2\n",
        "            # Method to get the number of output channels for this block\n",
        "    def get_out_channels(self):\n",
        "        return self.conv2[0].out_channels\n",
        "\n",
        "    # Method to set the number of output channels for this block\n",
        "    def set_out_channels(self, out_channels):\n",
        "        self.conv1[0].out_channels = out_channels\n",
        "        self.conv2[0].in_channels = out_channels\n",
        "        self.conv2[0].out_channels = out_channels\n"
      ],
      "metadata": {
        "id": "hdmXSDgKvT94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UnetDown\n",
        "\n",
        "This class is used for the encoder (downsampling) part of the U-Net. The encoder progressively down-samples the input image and extracts features from it. In this case, it consists of a series of two ResidualConvBlock layers followed by a MaxPool2d layer. ResidualConvBlock is not defined here but it's usually a convolutional block with a residual connection, a popular mechanism introduced in ResNet. The MaxPool2d layer is used for downsampling, reducing the spatial dimensions of the feature maps.\n",
        "\n",
        "The forward method takes an input tensor x, and applies the sequential model (consisting of two ResidualConvBlock layers and a MaxPool2d layer) to this input, returning the output tensor."
      ],
      "metadata": {
        "id": "AttG-jAGvshR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetDown, self).__init__()\n",
        "        \n",
        "        # Create a list of layers for the downsampling block\n",
        "        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n",
        "        layers = [ResidualConvBlock(in_channels, out_channels), ResidualConvBlock(out_channels, out_channels), nn.MaxPool2d(2)]\n",
        "        \n",
        "        # Use the layers to create a sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the sequential model and return the output\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "xQ6ByyLNwPf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UnetUp\n"
      ],
      "metadata": {
        "id": "dpDi_bQ6wTDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetUp, self).__init__()\n",
        "        \n",
        "        # Create a list of layers for the upsampling block\n",
        "        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "        ]\n",
        "        \n",
        "        # Use the layers to create a sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        \n",
        "        # Pass the concatenated tensor through the sequential model and return the output\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YCAsZ4RbwUpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EmbedFC"
      ],
      "metadata": {
        "id": "DLCR4OcUwiQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedFC, self).__init__()\n",
        "        '''\n",
        "        This class defines a generic one layer feed-forward neural network for embedding input data of\n",
        "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
        "        '''\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        # define the layers for the network\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "        ]\n",
        "        \n",
        "        # create a PyTorch sequential model consisting of the defined layers\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten the input tensor\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        # apply the model layers to the flattened tensor\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "a_oaynnOwk_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context U-net"
      ],
      "metadata": {
        "id": "16lA-1B7wzGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextUnet(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
        "        super(ContextUnet, self).__init__()\n",
        "\n",
        "        # number of input channels, number of intermediate feature maps and number of classes\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "        self.n_cfeat = n_cfeat\n",
        "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
        "\n",
        "        # Initialize the initial convolutional layer\n",
        "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
        "\n",
        "        # Initialize the down-sampling path of the U-Net with two levels\n",
        "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
        "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
        "        \n",
        "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
        "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
        "\n",
        "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
        "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
        "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
        "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
        "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
        "\n",
        "        # Initialize the up-sampling path of the U-Net with three levels\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample \n",
        "            nn.GroupNorm(8, 2 * n_feat), # normalize                        \n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
        "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
        "\n",
        "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "            nn.GroupNorm(8, n_feat), # normalize\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, c=None):\n",
        "        \"\"\"\n",
        "        x : (batch, n_feat, h, w) : input image\n",
        "        t : (batch, n_cfeat)      : time step\n",
        "        c : (batch, n_classes)    : context label\n",
        "        \"\"\"\n",
        "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
        "\n",
        "        # pass the input image through the initial convolutional layer\n",
        "        x = self.init_conv(x)\n",
        "        # pass the result through the down-sampling path\n",
        "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
        "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
        "        \n",
        "        # convert the feature maps to a vector and apply an activation\n",
        "        hiddenvec = self.to_vec(down2)\n",
        "        \n",
        "        # mask out context if context_mask == 1\n",
        "        if c is None:\n",
        "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
        "            \n",
        "        # embed context and timestep\n",
        "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
        "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
        "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
        "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
        "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
        "\n",
        "\n",
        "        up1 = self.up0(hiddenvec)\n",
        "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
        "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
        "        out = self.out(torch.cat((up3, x), 1))\n",
        "        return out"
      ],
      "metadata": {
        "id": "ayw1jia4vKNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEzaX0N0eCXW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extremely Minimalistic Implementation of DDPM\n",
        "\n",
        "https://arxiv.org/abs/2006.11239\n",
        "\n",
        "Everything is self contained. (Except for pytorch and torchvision... of course)\n",
        "\n",
        "run it with `python superminddpm.py`\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "\n",
        "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns pre-computed schedules for DDPM sampling, training process.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
        "\n",
        "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
        "    sqrt_beta_t = torch.sqrt(beta_t)\n",
        "    alpha_t = 1 - beta_t\n",
        "    log_alpha_t = torch.log(alpha_t)\n",
        "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
        "\n",
        "    sqrtab = torch.sqrt(alphabar_t)\n",
        "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
        "\n",
        "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
        "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
        "\n",
        "    return {\n",
        "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
        "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
        "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
        "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
        "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
        "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
        "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
        "    }\n",
        "\n",
        "\n",
        "blk = lambda ic, oc: nn.Sequential(\n",
        "    nn.Conv2d(ic, oc, 7, padding=3),\n",
        "    nn.BatchNorm2d(oc),\n",
        "    nn.LeakyReLU(),\n",
        ")\n",
        "\n",
        "\n",
        "class DummyEpsModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This should be unet-like, but let's don't think about the model too much :P\n",
        "    Basically, any universal R^n -> R^n model should work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channel: int) -> None:\n",
        "        super(DummyEpsModel, self).__init__()\n",
        "        self.conv = nn.Sequential(  # with batchnorm\n",
        "            blk(n_channel, 64),\n",
        "            blk(64, 128),\n",
        "            blk(128, 256),\n",
        "            blk(256, 512),\n",
        "            blk(512, 256),\n",
        "            blk(256, 128),\n",
        "            blk(128, 64),\n",
        "            nn.Conv2d(64, n_channel, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t) -> torch.Tensor:\n",
        "        # Lets think about using t later. In the paper, they used Tr-like positional embeddings.\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eps_model: nn.Module,\n",
        "        betas: Tuple[float, float],\n",
        "        n_T: int,\n",
        "        criterion: nn.Module = nn.MSELoss(),\n",
        "    ) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.eps_model = eps_model\n",
        "\n",
        "        # register_buffer allows us to freely access these tensors by name. It helps device placement.\n",
        "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
        "            self.register_buffer(k, v)\n",
        "\n",
        "        self.n_T = n_T\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Makes forward diffusion x_t, and tries to guess epsilon value from x_t using eps_model.\n",
        "        This implements Algorithm 1 in the paper.\n",
        "        \"\"\"\n",
        "\n",
        "        _ts = torch.randint(1, self.n_T, (x.shape[0],)).to(\n",
        "            x.device\n",
        "        )  # t ~ Uniform(0, n_T)\n",
        "        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n",
        "\n",
        "        x_t = (\n",
        "            self.sqrtab[_ts, None, None, None] * x\n",
        "            + self.sqrtmab[_ts, None, None, None] * eps\n",
        "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
        "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
        "\n",
        "        return self.criterion(eps, self.eps_model(x_t, _ts / self.n_T))\n",
        "\n",
        "    def sample(self, n_sample: int, size, device) -> torch.Tensor:\n",
        "\n",
        "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
        "\n",
        "        # This samples accordingly to Algorithm 2. It is exactly the same logic.\n",
        "        for i in range(self.n_T, 0, -1):\n",
        "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
        "            eps = self.eps_model(x_i, i / self.n_T)\n",
        "            x_i = (\n",
        "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
        "                + self.sqrt_beta_t[i] * z\n",
        "            )\n",
        "\n",
        "        return x_i\n",
        "\n",
        "\n",
        "def train_mnist(n_epoch: int = 100, device=\"cuda:0\") -> None:\n",
        "\n",
        "    ddpm = DDPM(eps_model=DummyEpsModel(1), betas=(1e-4, 0.02), n_T=1000)\n",
        "    ddpm.to(device)\n",
        "\n",
        "    tf = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))]\n",
        "    )\n",
        "\n",
        "    dataset = MNIST(\n",
        "        \"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=tf,\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=20)\n",
        "    optim = torch.optim.Adam(ddpm.parameters(), lr=2e-4)\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        ddpm.train()\n",
        "\n",
        "        pbar = tqdm(dataloader)\n",
        "        loss_ema = None\n",
        "        for x, _ in pbar:\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            loss = ddpm(x)\n",
        "            loss.backward()\n",
        "            if loss_ema is None:\n",
        "                loss_ema = loss.item()\n",
        "            else:\n",
        "                loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n",
        "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
        "            optim.step()\n",
        "\n",
        "        ddpm.eval()\n",
        "        with torch.no_grad():\n",
        "            xh = ddpm.sample(16, (1, 28, 28), device)\n",
        "            grid = make_grid(xh, nrow=4)\n",
        "            save_image(grid, f\"./contents/ddpm_sample_{i}.png\")\n",
        "\n",
        "            # save model\n",
        "            torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_mnist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgement\n"
      ],
      "metadata": {
        "id": "mCQV98Mpi1md"
      }
    }
  ]
}