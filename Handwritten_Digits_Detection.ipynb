{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverTwilight/Neural_Network/blob/master/Neural_Network_Full_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EzQhFI_imOU"
      },
      "source": [
        "# Overview\n",
        "\n",
        "> Created by **Rene Wang**\n",
        "\n",
        "This is a full example of how to detect hand-writting number with deep learning. The example is based on the MNIST dataset, which contains 60,000 training images and 10,000 testing images. Each image is a 28x28 grayscale image of a hand-written digit. The goal is to train a model to correctly classify the digit in the image.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/18B-Fujnr7uDhfyERZzWHTI3-31anw5OH?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwXl6HSPOKEN",
        "outputId": "f5599513-d101-4460-ba9d-ecbec44fd43c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!pip install colorama\n",
        "is_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdTZJ42evAVC"
      },
      "source": [
        "## Dataset Process\n",
        "\n",
        "This will transform the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). You can use this [script]() to generate augumented dataset with applying little translate and rotataion to the exsited data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mc8rwDOo2Ts"
      },
      "source": [
        "### Use Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t2zz9-LQUTq"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#@title Custom Data Config\n",
        "min_therehold = 127 #@param {type:\"number\"}\n",
        "max_therehold = 255 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYhjEh7dZmQz"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "train_data_path = '/content/drive/MyDrive/Project/NeuralNetwork/data/train'\n",
        "test_data_path = '/content/drive/MyDrive/Project/NeuralNetwork/data/test'\n",
        "\n",
        "def read_file(data_path, one_hot_label, flatten=False):\n",
        "    img_size = 784\n",
        "\n",
        "    x_train = []\n",
        "    t_train = []\n",
        "\n",
        "    for foldername in os.listdir(data_path):\n",
        "        bundle_path = os.path.join(data_path, foldername)\n",
        "        if os.path.isdir(bundle_path):\n",
        "            images = []  # list to store all images in the bundle\n",
        "            for file_name in os.listdir(bundle_path):\n",
        "\n",
        "                file_type = str(file_name.split('.')[1])\n",
        "                if file_type != \"png\" and file_type != \"jpg\":\n",
        "                    continue\n",
        "\n",
        "                img = cv2.imread(os.path.join(bundle_path, file_name))\n",
        "                label = int(file_name[0])\n",
        "\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                \n",
        "                retval, dst = cv2.threshold(gray, min_therehold, max_therehold, cv2.THRESH_OTSU)\n",
        "\n",
        "                dst = dst.astype(np.float32)\n",
        "\n",
        "                flattened_img = dst.flatten()\n",
        "                flattened_img = 255 - flattened_img\n",
        "\n",
        "                img_processed = flattened_img.reshape(dst.shape)\n",
        "                \n",
        "                images.append(cv2.resize(img_processed, (28, 28)))  # add resized image to bundle\n",
        "\n",
        "                flattened_img /= 255.0\n",
        "\n",
        "                x_train.append(flattened_img)\n",
        "\n",
        "                if one_hot_label:\n",
        "                    one_hot = [0] * 10\n",
        "                    one_hot[label] = 1\n",
        "                    t_train.append(one_hot)\n",
        "\n",
        "                else:\n",
        "                    t_train.append(label)\n",
        "                    \n",
        "            # concatenate images horizontally and display the result\n",
        "            if len(images) > 0:\n",
        "                print(bundle_path)\n",
        "                bundle_img = cv2.hconcat(images)\n",
        "                cv2_imshow(bundle_img)\n",
        "\n",
        "    x = np.array(x_train)\n",
        "    t = np.array(t_train)\n",
        "\n",
        "    return (x, t)\n",
        "\n",
        "def load_custom_data(one_hot_label=False):\n",
        "    return read_file(train_data_path, one_hot_label), read_file(test_data_path, one_hot_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx0UVnxyo3tX"
      },
      "source": [
        "### Use MINIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na81Nc7bvCN4",
        "outputId": "15343059-ce0f-4596-ba9d-15a150408d24"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# coding: utf-8\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n",
        "import sys, os\n",
        "sys.path.append('/content/')\n",
        "\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "# dataset_dir = os.path.dirname(os.path.abspath(\"/content\"))\n",
        "dataset_dir = \"/content\"\n",
        "dataset_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "    \n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "       _download(v)\n",
        "        \n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(dataset_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "        \n",
        "    return T\n",
        "    \n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"读入MNIST数据集\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 将图像的像素值正规化为0.0~1.0\n",
        "    one_hot_label : \n",
        "        one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "    flatten : 是否将图像展开为一维数组\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dataset_file):\n",
        "        init_mnist()\n",
        "        \n",
        "    with open(dataset_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "            \n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "    \n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init_mnist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjRQLf1HjZZY"
      },
      "source": [
        "# Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4IRMbnVNRwY"
      },
      "source": [
        "## Basic Funtions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnVM4bfSjk0t",
        "outputId": "d8c97e86-5f39-4c9d-9e64-caa348481acf"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Activation Function\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def step(x):\n",
        "    y = x > 0\n",
        "    return y.astype(int)\n",
        "\n",
        "print(step(np.array([1, 3, 0])))\n",
        "\n",
        "# Old and widely-used activation function.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# There are three common places of these three activation function:\n",
        "#\n",
        "#  1. The output is between 0 to 1\n",
        "#  2. Both is liner function\n",
        "#  3. The more important the input is, the bigger the output is.\n",
        "\n",
        "x = np.array([1, 2])\n",
        "w = np.array([[3, 4], [5, 2]]) # The row number should equal to x's length.\n",
        "\n",
        "# Diffrent operation order will output diffrenet result\n",
        "print(np.dot(w, x)) # [11, 9]\n",
        "print(np.dot(x, w)) # [13, 8] [1 x 3 + 2 x 5, 1 x 4 + 2 x 4]\n",
        "\n",
        "# Central Difference Derivation\n",
        "# We use 2 h to reduce the deviation.\n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4\n",
        "    return (f(x + h) - f(x - h)) / (2 * h)\n",
        "\n",
        "def func_1(x):\n",
        "    return 0.01 * x ** 2 + 0.1 * x\n",
        "\n",
        "x = np.arange(0.0, 20.0, 0.1)\n",
        "y = numerical_diff(func_1, x) # This is a valid operation (boardcast)\n",
        "\n",
        "try:\n",
        "    is_training\n",
        "except:\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.plot(x, y)\n",
        "    plt.show()\n",
        "\n",
        "def softmax(a):\n",
        "    c = np.max(a)\n",
        "    exp_a = np.exp(a - c) # e ^ (a - c)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "def softmax_batch(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 溢出对策\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "428IuPKjnPPa"
      },
      "source": [
        "## Cross Entropy Error\n",
        "\n",
        "We want the loss function result as small as possible.\n",
        "\n",
        "We introduce loss function to find a params that generate small loss function result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEBMqCA-nRWg"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    delta = 1e-7\n",
        "    return -np.sum(t * np.log(y + delta))\n",
        "    \n",
        "def cross_entropy_error_batch(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "    \n",
        "    # Only output the index of the right anwser.\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1) # 1 is the max\n",
        "    \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lnw14Dwl-hc"
      },
      "source": [
        "# Graident\n",
        "\n",
        "In vector calculus, the gradient of a scalar-valued differentiable function \n",
        "$ f $ of several variables is the vector field (or vector-valued function) f whose value at a point is the \"direction and rate of fastest increase\".\n",
        "\n",
        "The most basic way to find the gradient is to use the numerical differentiation method.\n",
        "\n",
        "$$  grad(x, y) = \\frac{f(x + h) - f(x - h)}{2h} $$\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        it.iternext()   \n",
        "\n",
        "def test_function(x):\n",
        "    return x[0] ** 2 + x[1] ** 2\n",
        "\n",
        "def gradient_desent(f, init_x, lr=0.01, step_num=100):\n",
        "    \"\"\"\n",
        "    lr is Learning Rate. This should not be too large or too small.\n",
        "    \"\"\"\n",
        "\n",
        "    x = init_x\n",
        "\n",
        "    for i in range(step_num):\n",
        "        grad = numerical_gradient(f, x)\n",
        "        x -= lr * grad\n",
        "\n",
        "    return x\n",
        "```\n",
        "\n",
        "There are four mainstream gradient desend algorithum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M27N5dUxQXKn"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val # 还原值\n",
        "        it.iternext()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6gWqIjrNrdg"
      },
      "source": [
        "## SGD (Stochastic Gradient Descent)\n",
        "\n",
        "We can get new weights by:\n",
        "\n",
        "$$ W \\leftarrow W - \\eta \\frac{\\delta L}{\\delta W} $$\n",
        "\n",
        "$ \\eta $ is the learning rate, and $ \\frac{\\delta L}{\\delta W} $ is the gradinent of $ W $\n",
        "\n",
        "The SGD sucks when the function is not anisotropic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXadNma1P0XQ"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        for key, val in params.items():\n",
        "            params[key] -= self.lr * grads[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmwLtamENzTh"
      },
      "source": [
        "## Momentum\n",
        "\n",
        "This method can make the gradient reach the extreme position faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSR-RnRXN2j_"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "    \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "            params[key] += self.v[key]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmXJIvTQFwR"
      },
      "source": [
        "## AdaGrad\n",
        "\n",
        "AdaGrad (Adaptive Gradient) is an algorithm for gradient-based optimization that adapts the learning rate component-wise to the parameters by incorporating knowledge of past observations. It introduces the **learning rate decay** method. Compared with the momentum method, this method will gradually reduce the learning rate.\n",
        "\n",
        "$$\\Delta w_t = - \\frac{\\eta}{\\sqrt{\\sum_{i=1}^{t} g_{i}^2 + \\epsilon}} g_t$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL6TPk60QiFL"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class AdaGrad:\n",
        "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.cache = {}\n",
        "\n",
        "    def update(self, params, gradients):\n",
        "        if not self.cache:\n",
        "            for key, value in params.items():\n",
        "                self.cache[key] = np.zeros_like(value)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.cache[key] += gradients[key] * gradients[key]\n",
        "            params[key] -= (self.learning_rate * gradients[key] / (np.sqrt(self.cache[key]) + self.epsilon))\n",
        "\n",
        "        return params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sOU-gn9YqEA"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81T5whX0YrnH"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class Adam:\n",
        "    def __init__():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnZJkBsNk8NQ"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O3UbAWBlY5f"
      },
      "source": [
        "## Relu\n",
        "\n",
        "Return x if x is larger than 0, otherwise return 0.\n",
        "\n",
        "$$\n",
        "Relu(x) = \\left\\{\n",
        "    \\begin{array}\\\\\n",
        "        1 & \\mbox{if } \\ x > 0 \\\\\n",
        "        0 & \\mbox{otherwise }\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWaHBc_BlFmK"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class Relu:\n",
        "    def __init__(self) -> None:\n",
        "        self.mask = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x should be a numpy array here\n",
        "        \"\"\"\n",
        "        self.mask = (x <= 0) # An array represting wheather each element is larger than 0. [True, False, False]\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        Set all the `Ture` in mask to 0\n",
        "        \"\"\"\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sas1Bu3nlPqI"
      },
      "source": [
        "## Affine\n",
        "\n",
        "$$\n",
        "Affine(x) = X • W + b\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_yAy2vhlR8T"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b) -> None:\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        dot = np.dot(self.x, self.W)\n",
        "        out = dot + self.b # Boardcasting...\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdcMMpfamwXf"
      },
      "source": [
        "## SoftmaxWithLoss\n",
        "\n",
        "The Softmax Loss is a widely used loss function in the field of deep learning. It is also referred to as the Cross-entropy loss with softmax.\n",
        "\n",
        "Let us consider the training data, denoted as 't', which is assumed to have undergone one-shot training, represented as follows:\n",
        "\n",
        " $$ t = (0, 0, 0, ..., 1, 0) $$\n",
        "\n",
        "Here, $ t_{k} $ corresponds to the correct answer. The predicted result, denoted by $ z $, can be expressed as:\n",
        "\n",
        " $$ z = (z_{1}, z_{2}, ..., z_{C}) $$\n",
        "\n",
        "The corresponding loss function, 'lz', can be formulated as:\n",
        "\n",
        " $$ l_{z} = \\sum_{i=1}^{C} t_{i} log(z) = -log(z_{k})$$\n",
        "\n",
        "In comparison to a linear function, the logarithmic function better represents our desired objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKcHjDbmyIJ"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self, print_result=False) -> None:\n",
        "        self.loss = None\n",
        "        self.print_result = print_result\n",
        "        self.y = None\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t \n",
        "        # Teaching Data. Marking the right answer.\n",
        "        # Set right anwser to 1 and wrongs to 0. For exmaple, [0, 0, 0, 1, 0, 0]\n",
        "\n",
        "        self.y = softmax_batch(x)\n",
        "        self.loss = cross_entropy_error_batch(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVQCcg1HWvr"
      },
      "source": [
        "## Batch Normalization\n",
        "\n",
        " $$ l_{B} \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m} x_{i} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9qF09xwHaU8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl7kCVngnOUY"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "Dropout layer will randomly delete some points. Since the network differs every time, it's similar with intergrate-learning(Use multiple models to learn and get the average)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chbap8VQnXK-"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class Dropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "    \n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            # Replace the item large than ratio. [True, True, False, ..., True]\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "        \n",
        "    def backward(self, dout):\n",
        "        # Same with Relu\n",
        "        return dout * self.mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F104OfzFpsmz"
      },
      "source": [
        "## Convolution "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIvPQLzHpuSo"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W):\n",
        "        self.W = W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-0ClQiqp3JO"
      },
      "source": [
        "## Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBySGUTJjy39"
      },
      "source": [
        "# Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVl0mOKYmKFr"
      },
      "source": [
        "## Multi-layer Net\n",
        "\n",
        "This network reach a accuarcy of 97%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUem1Q5Dj09y"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, weight_init_std=0.01, initParams=None, weight_decay_lambda=0, use_dropout=False, dropout_ratio=0) -> None:\n",
        "        \n",
        "        self.params = {}\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.hidden_layer_count = len(hidden_sizes)\n",
        "\n",
        "        if initParams is not None:\n",
        "            self.params = initParams\n",
        "        else:\n",
        "            self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_sizes[0])\n",
        "            self.params['b1'] = np.zeros(hidden_sizes[0])\n",
        "\n",
        "            for i in range(1, self.hidden_layer_count):\n",
        "                self.params[f'W{i+1}'] = weight_init_std * np.random.randn(hidden_sizes[i-1], hidden_sizes[i])\n",
        "                self.params[f'b{i+1}'] = np.zeros(hidden_sizes[i])\n",
        "\n",
        "            self.params[f'W{self.hidden_layer_count+1}'] = weight_init_std * np.random.randn(hidden_sizes[-1], output_size)\n",
        "            self.params[f'b{self.hidden_layer_count+1}'] = np.zeros(output_size)\n",
        "\n",
        "        self.layers = OrderedDict()\n",
        "        \n",
        "        for i in range(self.hidden_layer_count):\n",
        "            self.layers[f'Affine{i+1}'] = Affine(self.params[f'W{i+1}'], self.params[f'b{i+1}'])\n",
        "            self.layers[f'Relu{i+1}'] = Relu()\n",
        "            \n",
        "            if use_dropout:\n",
        "                self.layers[f'Dropout{i+1}'] = Dropout(dropout_ratio)\n",
        "\n",
        "        self.layers[f'Affine{self.hidden_layer_count+1}'] = Affine(self.params[f'W{self.hidden_layer_count+1}'], self.params[f'b{self.hidden_layer_count+1}'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss(print_result = (not initParams == None))\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for key, layer in self.layers.items():\n",
        "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x, train_flg=False)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        \n",
        "        # Get the index of the maximum value. If one-shot is enabled the max value is 1\n",
        "        # For example, [[1, 0, 0], [0,0,1]] will be converted to [0, 2]\n",
        "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
        "\n",
        "        # if x.shape[0] <= 50:\n",
        "        #     print(\"Expected Anwser: \" + str(t))\n",
        "        #     print(\"Exact Anwser: \" + str(y))\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "\n",
        "        return accuracy\n",
        "    \n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "    \n",
        "    def gradient(self, x, t):\n",
        "        self.loss(x, t)\n",
        "\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        grads = {}\n",
        "        for i in range(self.hidden_layer_count + 1):\n",
        "            grads[f\"W{i+1}\"] = self.layers[f\"Affine{i+1}\"].dW + self.weight_decay_lambda * self.layers[f\"Affine{i+1}\"].W\n",
        "            grads[f\"b{i+1}\"] = self.layers[f\"Affine{i+1}\"].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ikKQHFmTLT"
      },
      "source": [
        "## CNN\n",
        "\n",
        "Convolutional neural networks is an enhanced version, which offers significant advantages, most notably the preservation of data shape. In previous neural network architectures, it was often necessary to convert two-dimensional arrays to one-dimensional arrays. \n",
        "\n",
        "With CNN, however, original-shaped data can be directly inputted into the network, without the need for additional preprocessing steps. As a result, CNNs outperform other neural network architectures, particularly when processing colored images.\n",
        "\n",
        "$\\nabla F=\\begin{pmatrix} yz \\ xz \\ xy \\end{pmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGTcZ_vTsQnY"
      },
      "source": [
        "# Training\n",
        "\n",
        "It's unusual for the accuracy to plateau after only a few epochs, especially if you're using a relatively large dataset.\n",
        "\n",
        "## Optimization\n",
        "\n",
        "* Use Dropout\n",
        "* Use weight decay\n",
        "* Enlarge the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b58Yf86LP0q4"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#@title Training Config { run: \"auto\" }\n",
        "\n",
        "scene = \"Train\" #@param [\"Train\", \"Production\"]\n",
        "network_type = \"MultiLayer\" #@param [\"MultiLayer\", \"CNN\"]\n",
        "learning_rate = 0.01 #@param {type:\"number\"}\n",
        "batch_size =100 #@param {type:\"number\"}\n",
        "hidden_layers = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "layer_size = 100 #@param {type:\"number\"}\n",
        "iters_num = 25000 #@param {type:\"slider\", min:5000, max:100000, step:5000}\n",
        "optimizer_type = \"Momentum\" #@param [\"SGD\", \"Momentum\", \"AdaGrad\", \"Adam\"]\n",
        "train_data_source = \"MINIST + Custom\" #@param [\"MINIST\", \"MINIST + Custom\"]\n",
        "test_data_source = \"Custom\" #@param [\"MINIST\", \"Custom\", \"MINIST + Custom\"]\n",
        "weight_decay_lambda = 0.05 #@param {type:\"slider\", min:0, max:0.3, step:0.01}\n",
        "use_dropout = False #@param {type:\"boolean\"}\n",
        "dropout_ratio = 0.1 #@param {type:\"slider\", min:0, max:0.3, step:0.01}\n",
        "load_params = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pHkHWIfEssBH",
        "outputId": "fc133252-f0e3-458b-8f18-ddcf47cd42d8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '\"c:'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from colorama import Fore, Back, Style\n",
        "import math\n",
        "\n",
        "hidden_sizes = [layer_size for _ in range(hidden_layers)]\n",
        "\n",
        "def train(x_train, t_train, x_test, t_test, initParams=None):\n",
        "\n",
        "    network = TwoLayerNet(input_size=784, hidden_sizes=hidden_sizes, output_size=10, initParams=initParams, weight_decay_lambda=weight_decay_lambda, use_dropout=use_dropout, dropout_ratio=dropout_ratio)\n",
        "\n",
        "    optimizer = None\n",
        "\n",
        "    if optimizer_type == \"SGD\":\n",
        "        optimizer = SGD(lr=learning_rate)\n",
        "    elif optimizer_type == \"Momentum\":\n",
        "        optimizer = Momentum(lr=learning_rate)\n",
        "    elif optimizer_type == \"AdaGard\":\n",
        "        optimizer = SGD()\n",
        "    else:\n",
        "        optimizer = SGD()\n",
        "\n",
        "    train_size = x_train.shape[0] # 60000\n",
        "    train_loss_list = []\n",
        "    train_acc_list = []\n",
        "    test_acc_list = []\n",
        "\n",
        "    iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "    for i in range(iters_num):\n",
        "        batch_mask = np.random.choice(train_size, batch_size) # Select a batch_size between 0 - train_size\n",
        "\n",
        "        # Randomly select a part of data\n",
        "        x_batch = x_train[batch_mask]\n",
        "        t_batch = t_train[batch_mask]\n",
        "\n",
        "        if not initParams:\n",
        "            grads = network.gradient(x_batch, t_batch)\n",
        "            optimizer.update(network.params, grads)\n",
        "\n",
        "        loss = network.loss(x_batch, t_batch)\n",
        "        train_loss_list.append(loss)\n",
        "\n",
        "        # Only calcuate accuracy every epoch. All data passed in.\n",
        "        if i % math.floor(iter_per_epoch) == 0:\n",
        "            train_acc = network.accuracy(x_train, t_train)\n",
        "            test_acc = network.accuracy(x_test, t_test)\n",
        "            train_acc_list.append(train_acc)\n",
        "            test_acc_list.append(test_acc)\n",
        "            # print(train_acc, test_acc)\n",
        "    \n",
        "    epochs = range(len(train_acc_list))\n",
        "    plt.plot(epochs, train_acc_list, label='Train Accuracy')\n",
        "    plt.plot(epochs, test_acc_list, label='Test Accuracy')\n",
        "    plt.title('Accuracy Change Per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # print(\"Max Test Accuracy: \" + Fore.BLUE + str(np.maximum(test_acc_list)) + Fore.reset)\n",
        "    \n",
        "    return network.params\n",
        "\n",
        "param_cache = None\n",
        "param_cache_dir = \"/content\"\n",
        "param_cache_file = param_cache_dir + \"/params.pkl\"\n",
        "\n",
        "if os.path.exists(param_cache_file):\n",
        "    with open(param_cache_file, 'rb') as f:\n",
        "        param_cache = pickle.load(f)\n",
        "\n",
        "minist_dataset = load_mnist(normalize=True, one_hot_label=True)\n",
        "custom_dataset = load_custom_data(one_hot_label=True)\n",
        "\n",
        "if train_data_source == \"MINIST\":\n",
        "    (x_train, t_train), _ = minist_dataset\n",
        "elif train_data_source == \"MINIST + Custom\":\n",
        "    (x_train_mnist, t_train_mnist), _ = minist_dataset\n",
        "    (x_train_custom, t_train_custom), _ = custom_dataset\n",
        "    x_train = np.concatenate((x_train_mnist, x_train_custom), axis=0)\n",
        "    t_train = np.concatenate((t_train_mnist, t_train_custom), axis=0)\n",
        "\n",
        "if test_data_source == \"MINIST\":\n",
        "    _, (x_test, t_test) = minist_dataset\n",
        "elif test_data_source == \"MINIST + Custom\":\n",
        "    _, (x_test_mnist, t_test_mnist) = minist_dataset\n",
        "    _, (x_test_custom, t_test_custom) = custom_dataset\n",
        "    x_test = np.concatenate((x_test_mnist, x_test_custom), axis=0)\n",
        "    t_test = np.concatenate((t_test_mnist, t_test_custom), axis=0)\n",
        "elif test_data_source == \"Custom\":\n",
        "    _ , (x_test, t_test) = custom_dataset\n",
        "\n",
        "print(Back.GREEN + \"● Entering \" + scene +  \" Mode...\" + Back.RESET)\n",
        "\n",
        "if scene == \"Train\":\n",
        "    initParams = None\n",
        "\n",
        "    if load_params and param_cache:\n",
        "        initParams = param_cache\n",
        "    \n",
        "    print(\"x_train size: \" + Fore.CYAN + str(x_train.shape) + Fore.RESET)\n",
        "    print(\"x_test size: \" + Fore.CYAN + str(x_test.shape) + Fore.RESET)\n",
        "    \n",
        "    params = train(x_train, t_train, x_test, t_test, initParams)\n",
        "\n",
        "    if not load_params or not os.path.exists(param_cache_file):\n",
        "        with open(param_cache_file, 'wb') as f:\n",
        "            pickle.dump(params, f, -1)\n",
        "            \n",
        "elif param_cache:\n",
        "    network = TwoLayerNet(input_size=784, hidden_sizes=hidden_sizes, output_size=10, initParams=param_cache, weight_decay_lambda=weight_decay_lambda)\n",
        "\n",
        "    print(\"Results: \" + Fore.YELLOW + str(np.argmax(network.predict(x_test), axis=1)) + Fore.RESET)\n",
        "    print(\"Expects: \" + Fore.GREEN + str(np.argmax(t_test, axis=1)))\n",
        "    \n",
        "else:\n",
        "    print(Fore.YELLOW + \"Please train params first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFggngb-Zdlh"
      },
      "source": [
        "## Reference\n",
        "\n",
        "1. Saito Yasuhiro. Deep Learning from Scratch[M]. Japan: O'Reilly Japan, 2016.\n",
        "\n",
        "2. 管他叫大靖. (2021年05月24日). Softmax Loss 的推导及改进. 知乎专栏. (https://zhuanlan.zhihu.com/p/374018199).\n",
        "\n",
        "3. Khelifi Ahmed Aziz. Medium. Learn How to Write Markdown & LaTeX in The Jupyter Notebook (https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)\n",
        "\n",
        "3. Jay Gupta. Medium. Going beyond 99% — MNIST Handwritten Digits Recognition (https://towardsdatascience.com/going-beyond-99-mnist-handwritten-digits-recognition-cfff96337392)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyO/yw1/LLBQwwj444gf7I4K",
      "collapsed_sections": [
        "RjRQLf1HjZZY",
        "428IuPKjnPPa",
        "V4ikKQHFmTLT"
      ],
      "include_colab_link": true,
      "mount_file_id": "18B-Fujnr7uDhfyERZzWHTI3-31anw5OH",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
