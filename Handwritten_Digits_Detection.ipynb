{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "outputs": [],
   "source": [
    "<a href=\"https://colab.research.google.com/github/RiverTwilight/Neural_Network/blob/master/Neural_Network_Full_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EzQhFI_imOU"
   },
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "> 作者： **Rene Wang**\n",
    "\n",
    "这是一个完整的示例，演示了如何使用深度学习检测手写数字。该示例基于 MNIST 数据集，该数据集包含 60,000 张训练图像和 10,000 张测试图像。每个图像都是一个 28x28 的灰度图，表示手写的数字。目标是训练一个模型，正确分类图像中的数字。\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/18B-Fujnr7uDhfyERZzWHTI3-31anw5OH?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwXl6HSPOKEN",
    "outputId": "f5599513-d101-4460-ba9d-ecbec44fd43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: colorama in c:\\files\\projects\\python\\neural_network\\env\\lib\\site-packages (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install colorama\n",
    "is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdTZJ42evAVC"
   },
   "outputs": [],
   "source": [
    "## 数据集处理\n",
    "\n",
    "这将把图像的格式从一个二维数组（28x28 像素）转换为一个一维数组（28 * 28 = 784 像素）。您可以使用此 [脚本]() 生成增强数据集，对现有数据进行轻微的平移和旋转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Mc8rwDOo2Ts"
   },
   "outputs": [],
   "source": [
    "### 使用自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-t2zz9-LQUTq"
   },
   "outputs": [],
   "source": [
    "#@title 自定义数据配置\n",
    "min_therehold = 127 #@param {type:\"number\"}\n",
    "max_therehold = 255 #@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tYhjEh7dZmQz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "train_data_path = '/dataset/train'\n",
    "test_data_path = '/content/test'\n",
    "\n",
    "def read_file(data_path, one_hot_label, flatten=False):\n",
    "    img_size = 784\n",
    "\n",
    "    x_train = []\n",
    "    t_train = []\n",
    "\n",
    "    for foldername in os.listdir(data_path):\n",
    "        bundle_path = os.path.join(data_path, foldername)\n",
    "        if os.path.isdir(bundle_path):\n",
    "            images = []  # 用于存储捆绑中所有图像的列表\n",
    "            for file_name in os.listdir(bundle_path):\n",
    "\n",
    "                file_type = str(file_name.split('.')[1])\n",
    "                if file_type != \"png\" and file_type != \"jpg\":\n",
    "                    continue\n",
    "\n",
    "                img = cv2.imread(os.path.join(bundle_path, file_name))\n",
    "                label = int(file_name[0])\n",
    "\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                retval, dst = cv2.threshold(gray, min_therehold, max_therehold, cv2.THRESH_OTSU)\n",
    "\n",
    "                dst = dst.astype(np.float32)\n",
    "\n",
    "                flattened_img = dst.flatten()\n",
    "                flattened_img = 255 - flattened_img\n",
    "\n",
    "                img_processed = flattened_img.reshape(dst.shape)\n",
    "                \n",
    "                images.append(cv2.resize(img_processed, (28, 28)))  # add resized image to bundle\n",
    "\n",
    "                flattened_img /= 255.0\n",
    "\n",
    "                x_train.append(flattened_img)\n",
    "\n",
    "                if one_hot_label:\n",
    "                    one_hot = [0] * 10\n",
    "                    one_hot[label] = 1\n",
    "                    t_train.append(one_hot)\n",
    "\n",
    "                else:\n",
    "                    t_train.append(label)\n",
    "                    \n",
    "            # concatenate images horizontally and display the result\n",
    "            if len(images) > 0:\n",
    "                print(bundle_path)\n",
    "                bundle_img = cv2.hconcat(images)\n",
    "                cv2.imshow(bundle_img)\n",
    "                # cv2_imshow(bundle_img)\n",
    "\n",
    "    x = np.array(x_train)\n",
    "    t = np.array(t_train)\n",
    "\n",
    "    return (x, t)\n",
    "\n",
    "def load_custom_data(one_hot_label=False):\n",
    "    return read_file(train_data_path, one_hot_label), read_file(test_data_path, one_hot_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx0UVnxyo3tX"
   },
   "outputs": [],
   "source": [
    "### 使用 MNIST 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Na81Nc7bvCN4",
    "outputId": "15343059-ce0f-4596-ba9d-15a150408d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dataset/train-images-idx3-ubyte.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b818257da987>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0minit_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-b818257da987>\u001b[0m in \u001b[0;36minit_mnist\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mdownload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating pickle file ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-b818257da987>\u001b[0m in \u001b[0;36mdownload_mnist\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdownload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m        \u001b[0m_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_load_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-b818257da987>\u001b[0m in \u001b[0;36m_download\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" ... \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_base\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# Handle temporary file setup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dataset/train-images-idx3-ubyte.gz'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append('./')\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "# dataset_dir = os.path.dirname(os.path.abspath(\"/content\"))\n",
    "dataset_dir = \"/dataset\"\n",
    "dataset_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(dataset_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"读入MNIST数据集\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 将图像的像素值正规化为0.0~1.0\n",
    "    one_hot_label : \n",
    "        one_hot_label为True的情况下，标签作为one-hot数组返回\n",
    "        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
    "    flatten : 是否将图像展开为一维数组\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (训练图像, 训练标签), (测试图像, 测试标签)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dataset_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(dataset_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjRQLf1HjZZY"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4IRMbnVNRwY"
   },
   "outputs": [],
   "source": [
    "## Basic Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnVM4bfSjk0t",
    "outputId": "d8c97e86-5f39-4c9d-9e64-caa348481acf"
   },
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def step(x):\n",
    "    y = x > 0\n",
    "    return y.astype(int)\n",
    "\n",
    "print(step(np.array([1, 3, 0])))\n",
    "\n",
    "# 旧的广泛使用的激活函数。\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# 这三种激活函数有三个常见的特点：\n",
    "#\n",
    "#  1. 输出介于 0 到 1 之间\n",
    "#  2. 都是线性函数\n",
    "#  3. 输入越重要，输出越大。\n",
    "\n",
    "x = np.array([1, 2])\n",
    "w = np.array([[3, 4], [5, 2]]) # 行数应该等于 x 的长度\n",
    "\n",
    "# 不同的操作顺序会产生不同的结果\n",
    "print(np.dot(w, x)) # [11, 9]\n",
    "print(np.dot(x, w)) # [13, 8] [1 x 3 + 2 x 5, 1 x 4 + 2 x 4]\n",
    "\n",
    "# 中心差分推导\n",
    "# 我们使用 2h 来减小偏差。\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def func_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = numerical_diff(func_1, x) # 这是一个有效的操作（广播）\n",
    "\n",
    "try:\n",
    "    is_training\n",
    "except:\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # e ^ (a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y\n",
    "\n",
    "def softmax_batch(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 溢出对策\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "428IuPKjnPPa"
   },
   "outputs": [],
   "source": [
    "## 交叉熵误差\n",
    "\n",
    "我们希望损失函数的结果尽可能小。\n",
    "\n",
    "我们引入损失函数来找到生成较小损失函数结果的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEBMqCA-nRWg"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "    \n",
    "def cross_entropy_error_batch(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    # 只输出正确答案的索引。\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1) # 1 is the max\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Lnw14Dwl-hc"
   },
   "outputs": [],
   "source": [
    "# 梯度\n",
    "\n",
    "在向量微积分中，对于多个变量的标量可微函数 $ f $，梯度是指函数 f 的向量场（或向量值函数），在某一点的值是“最快增长的方向和速率”。\n",
    "\n",
    "找到梯度最基本的方法是使用数值微分方法。\n",
    "\n",
    "$$  grad(x, y) = \\frac{f(x + h) - f(x - h)}{2h} $$\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原值\n",
    "        it.iternext()   \n",
    "\n",
    "def test_function(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "def gradient_desent(f, init_x, lr=0.01, step_num=100):\n",
    "    \"\"\"\n",
    "    lr 是学习率。它既不应该太大也不应该太小。\n",
    "    \"\"\"\n",
    "\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "有四种主流的梯度下降算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M27N5dUxQXKn"
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 还原值\n",
    "        it.iternext()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6gWqIjrNrdg"
   },
   "outputs": [],
   "source": [
    "## SGD（随机梯度下降）\n",
    "\n",
    "我们可以通过以下方式获得新的权重：\n",
    "\n",
    "$$ W \\leftarrow W - \\eta \\frac{\\delta L}{\\delta W} $$\n",
    "\n",
    "其中 $ \\eta $ 是学习率，$ \\frac{\\delta L}{\\delta W} $ 是 $ W $ 的梯度。\n",
    "\n",
    "当函数不是各向异性时，SGD 效果不佳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXadNma1P0XQ"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key, val in params.items():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmwLtamENzTh"
   },
   "outputs": [],
   "source": [
    "## 动量法\n",
    "\n",
    "这种方法可以使梯度更快地到达极值位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSR-RnRXN2j_"
   },
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvmXJIvTQFwR"
   },
   "outputs": [],
   "source": [
    "## AdaGrad\n",
    "\n",
    "AdaGrad（自适应梯度）是一种基于梯度的优化算法，通过结合对过去观察的知识，逐个参数地调整学习率。它引入了**学习率衰减**方法。与动量法相比，这种方法会逐渐降低学习率。\n",
    "\n",
    "$$\\Delta w_t = - \\frac{\\eta}{\\sqrt{\\sum_{i=1}^{t} g_{i}^2 + \\epsilon}} g_t$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zL6TPk60QiFL"
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = {}\n",
    "\n",
    "    def update(self, params, gradients):\n",
    "        if not self.cache:\n",
    "            for key, value in params.items():\n",
    "                self.cache[key] = np.zeros_like(value)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.cache[key] += gradients[key] * gradients[key]\n",
    "            params[key] -= (self.learning_rate * gradients[key] / (np.sqrt(self.cache[key]) + self.epsilon))\n",
    "\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sOU-gn9YqEA"
   },
   "outputs": [],
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81T5whX0YrnH"
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnZJkBsNk8NQ"
   },
   "outputs": [],
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O3UbAWBlY5f"
   },
   "outputs": [],
   "source": [
    "## Relu\n",
    "\n",
    "Return x if x is larger than 0, otherwise return 0.\n",
    "\n",
    "$$\n",
    "Relu(x) = \\left\\{\n",
    "    \\begin{array}\\\\\n",
    "        1 & \\mbox{if } \\ x > 0 \\\\\n",
    "        0 & \\mbox{otherwise }\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWaHBc_BlFmK"
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self) -> None:\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x should be a numpy array here\n",
    "        \"\"\"\n",
    "        self.mask = (x <= 0) # An array represting wheather each element is larger than 0. [True, False, False]\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Set all the `Ture` in mask to 0\n",
    "        \"\"\"\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sas1Bu3nlPqI"
   },
   "outputs": [],
   "source": [
    "## Affine\n",
    "\n",
    "$$\n",
    "Affine(x) = X • W + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_yAy2vhlR8T"
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b) -> None:\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        dot = np.dot(self.x, self.W)\n",
    "        out = dot + self.b # Boardcasting...\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdcMMpfamwXf"
   },
   "outputs": [],
   "source": [
    "## SoftmaxWithLoss\n",
    "\n",
    "The Softmax Loss is a widely used loss function in the field of deep learning. It is also referred to as the Cross-entropy loss with softmax.\n",
    "\n",
    "Let us consider the training data, denoted as 't', which is assumed to have undergone one-shot training, represented as follows:\n",
    "\n",
    " $$ t = (0, 0, 0, ..., 1, 0) $$\n",
    "\n",
    "Here, $ t_{k} $ corresponds to the correct answer. The predicted result, denoted by $ z $, can be expressed as:\n",
    "\n",
    " $$ z = (z_{1}, z_{2}, ..., z_{C}) $$\n",
    "\n",
    "The corresponding loss function, 'lz', can be formulated as:\n",
    "\n",
    " $$ l_{z} = \\sum_{i=1}^{C} t_{i} log(z) = -log(z_{k})$$\n",
    "\n",
    "In comparison to a linear function, the logarithmic function better represents our desired objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tKcHjDbmyIJ"
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self, print_result=False) -> None:\n",
    "        self.loss = None\n",
    "        self.print_result = print_result\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t \n",
    "        # Teaching Data. Marking the right answer.\n",
    "        # Set right anwser to 1 and wrongs to 0. For exmaple, [0, 0, 0, 1, 0, 0]\n",
    "\n",
    "        self.y = softmax_batch(x)\n",
    "        self.loss = cross_entropy_error_batch(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuVQCcg1HWvr"
   },
   "outputs": [],
   "source": [
    "## Batch Normalization\n",
    "\n",
    " $$ l_{B} \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m} x_{i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9qF09xwHaU8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wl7kCVngnOUY"
   },
   "outputs": [],
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout layer will randomly delete some points. Since the network differs every time, it's similar with intergrate-learning(Use multiple models to learn and get the average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Chbap8VQnXK-"
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            # Replace the item large than ratio. [True, True, False, ..., True]\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        # Same with Relu\n",
    "        return dout * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F104OfzFpsmz"
   },
   "outputs": [],
   "source": [
    "## Convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIvPQLzHpuSo"
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W):\n",
    "        self.W = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-0ClQiqp3JO"
   },
   "outputs": [],
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBySGUTJjy39"
   },
   "outputs": [],
   "source": [
    "# Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVl0mOKYmKFr"
   },
   "outputs": [],
   "source": [
    "## Multi-layer Net\n",
    "\n",
    "This network reach a accuarcy of 97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUem1Q5Dj09y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, weight_init_std=0.01, initParams=None, weight_decay_lambda=0, use_dropout=False, dropout_ratio=0) -> None:\n",
    "        \n",
    "        self.params = {}\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.hidden_layer_count = len(hidden_sizes)\n",
    "\n",
    "        if initParams is not None:\n",
    "            self.params = initParams\n",
    "        else:\n",
    "            self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_sizes[0])\n",
    "            self.params['b1'] = np.zeros(hidden_sizes[0])\n",
    "\n",
    "            for i in range(1, self.hidden_layer_count):\n",
    "                self.params[f'W{i+1}'] = weight_init_std * np.random.randn(hidden_sizes[i-1], hidden_sizes[i])\n",
    "                self.params[f'b{i+1}'] = np.zeros(hidden_sizes[i])\n",
    "\n",
    "            self.params[f'W{self.hidden_layer_count+1}'] = weight_init_std * np.random.randn(hidden_sizes[-1], output_size)\n",
    "            self.params[f'b{self.hidden_layer_count+1}'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        for i in range(self.hidden_layer_count):\n",
    "            self.layers[f'Affine{i+1}'] = Affine(self.params[f'W{i+1}'], self.params[f'b{i+1}'])\n",
    "            self.layers[f'Relu{i+1}'] = Relu()\n",
    "            \n",
    "            if use_dropout:\n",
    "                self.layers[f'Dropout{i+1}'] = Dropout(dropout_ratio)\n",
    "\n",
    "        self.layers[f'Affine{self.hidden_layer_count+1}'] = Affine(self.params[f'W{self.hidden_layer_count+1}'], self.params[f'b{self.hidden_layer_count+1}'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss(print_result = (not initParams == None))\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, train_flg=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "        # Get the index of the maximum value. If one-shot is enabled the max value is 1\n",
    "        # For example, [[1, 0, 0], [0,0,1]] will be converted to [0, 2]\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        # if x.shape[0] <= 50:\n",
    "        #     print(\"Expected Anwser: \" + str(t))\n",
    "        #     print(\"Exact Anwser: \" + str(y))\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(self.hidden_layer_count + 1):\n",
    "            grads[f\"W{i+1}\"] = self.layers[f\"Affine{i+1}\"].dW + self.weight_decay_lambda * self.layers[f\"Affine{i+1}\"].W\n",
    "            grads[f\"b{i+1}\"] = self.layers[f\"Affine{i+1}\"].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4ikKQHFmTLT"
   },
   "outputs": [],
   "source": [
    "## CNN\n",
    "\n",
    "Convolutional neural networks is an enhanced version, which offers significant advantages, most notably the preservation of data shape. In previous neural network architectures, it was often necessary to convert two-dimensional arrays to one-dimensional arrays. \n",
    "\n",
    "With CNN, however, original-shaped data can be directly inputted into the network, without the need for additional preprocessing steps. As a result, CNNs outperform other neural network architectures, particularly when processing colored images.\n",
    "\n",
    "$\\nabla F=\\begin{pmatrix} yz \\ xz \\ xy \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGTcZ_vTsQnY"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "It's unusual for the accuracy to plateau after only a few epochs, especially if you're using a relatively large dataset.\n",
    "\n",
    "## Optimization\n",
    "\n",
    "* Use Dropout\n",
    "* Use weight decay\n",
    "* Enlarge the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b58Yf86LP0q4"
   },
   "outputs": [],
   "source": [
    "#@title Training Config { run: \"auto\" }\n",
    "\n",
    "scene = \"Train\" #@param [\"Train\", \"Production\"]\n",
    "network_type = \"MultiLayer\" #@param [\"MultiLayer\", \"CNN\"]\n",
    "learning_rate = 0.01 #@param {type:\"number\"}\n",
    "batch_size =100 #@param {type:\"number\"}\n",
    "hidden_layers = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n",
    "layer_size = 100 #@param {type:\"number\"}\n",
    "iters_num = 25000 #@param {type:\"slider\", min:5000, max:100000, step:5000}\n",
    "optimizer_type = \"Momentum\" #@param [\"SGD\", \"Momentum\", \"AdaGrad\", \"Adam\"]\n",
    "train_data_source = \"MINIST + Custom\" #@param [\"MINIST\", \"MINIST + Custom\"]\n",
    "test_data_source = \"Custom\" #@param [\"MINIST\", \"Custom\", \"MINIST + Custom\"]\n",
    "weight_decay_lambda = 0.05 #@param {type:\"slider\", min:0, max:0.3, step:0.01}\n",
    "use_dropout = False #@param {type:\"boolean\"}\n",
    "dropout_ratio = 0.1 #@param {type:\"slider\", min:0, max:0.3, step:0.01}\n",
    "load_params = False #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pHkHWIfEssBH",
    "outputId": "fc133252-f0e3-458b-8f18-ddcf47cd42d8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore, Back, Style\n",
    "import math\n",
    "\n",
    "hidden_sizes = [layer_size for _ in range(hidden_layers)]\n",
    "\n",
    "def train(x_train, t_train, x_test, t_test, initParams=None):\n",
    "\n",
    "    network = TwoLayerNet(input_size=784, hidden_sizes=hidden_sizes, output_size=10, initParams=initParams, weight_decay_lambda=weight_decay_lambda, use_dropout=use_dropout, dropout_ratio=dropout_ratio)\n",
    "\n",
    "    optimizer = None\n",
    "\n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer = SGD(lr=learning_rate)\n",
    "    elif optimizer_type == \"Momentum\":\n",
    "        optimizer = Momentum(lr=learning_rate)\n",
    "    elif optimizer_type == \"AdaGard\":\n",
    "        optimizer = SGD()\n",
    "    else:\n",
    "        optimizer = SGD()\n",
    "\n",
    "    train_size = x_train.shape[0] # 60000\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "    for i in range(iters_num):\n",
    "        batch_mask = np.random.choice(train_size, batch_size) # Select a batch_size between 0 - train_size\n",
    "\n",
    "        # Randomly select a part of data\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "\n",
    "        if not initParams:\n",
    "            grads = network.gradient(x_batch, t_batch)\n",
    "            optimizer.update(network.params, grads)\n",
    "\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        # Only calcuate accuracy every epoch. All data passed in.\n",
    "        if i % math.floor(iter_per_epoch) == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            test_acc = network.accuracy(x_test, t_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            # print(train_acc, test_acc)\n",
    "    \n",
    "    epochs = range(len(train_acc_list))\n",
    "    plt.plot(epochs, train_acc_list, label='Train Accuracy')\n",
    "    plt.plot(epochs, test_acc_list, label='Test Accuracy')\n",
    "    plt.title('Accuracy Change Per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # print(\"Max Test Accuracy: \" + Fore.BLUE + str(np.maximum(test_acc_list)) + Fore.reset)\n",
    "    \n",
    "    return network.params\n",
    "\n",
    "param_cache = None\n",
    "param_cache_dir = \"/content\"\n",
    "param_cache_file = param_cache_dir + \"/params.pkl\"\n",
    "\n",
    "if os.path.exists(param_cache_file):\n",
    "    with open(param_cache_file, 'rb') as f:\n",
    "        param_cache = pickle.load(f)\n",
    "\n",
    "minist_dataset = load_mnist(normalize=True, one_hot_label=True)\n",
    "custom_dataset = load_custom_data(one_hot_label=True)\n",
    "\n",
    "if train_data_source == \"MINIST\":\n",
    "    (x_train, t_train), _ = minist_dataset\n",
    "elif train_data_source == \"MINIST + Custom\":\n",
    "    (x_train_mnist, t_train_mnist), _ = minist_dataset\n",
    "    (x_train_custom, t_train_custom), _ = custom_dataset\n",
    "    x_train = np.concatenate((x_train_mnist, x_train_custom), axis=0)\n",
    "    t_train = np.concatenate((t_train_mnist, t_train_custom), axis=0)\n",
    "\n",
    "if test_data_source == \"MINIST\":\n",
    "    _, (x_test, t_test) = minist_dataset\n",
    "elif test_data_source == \"MINIST + Custom\":\n",
    "    _, (x_test_mnist, t_test_mnist) = minist_dataset\n",
    "    _, (x_test_custom, t_test_custom) = custom_dataset\n",
    "    x_test = np.concatenate((x_test_mnist, x_test_custom), axis=0)\n",
    "    t_test = np.concatenate((t_test_mnist, t_test_custom), axis=0)\n",
    "elif test_data_source == \"Custom\":\n",
    "    _ , (x_test, t_test) = custom_dataset\n",
    "\n",
    "print(Back.GREEN + \"● Entering \" + scene +  \" Mode...\" + Back.RESET)\n",
    "\n",
    "if scene == \"Train\":\n",
    "    initParams = None\n",
    "\n",
    "    if load_params and param_cache:\n",
    "        initParams = param_cache\n",
    "    \n",
    "    print(\"x_train size: \" + Fore.CYAN + str(x_train.shape) + Fore.RESET)\n",
    "    print(\"x_test size: \" + Fore.CYAN + str(x_test.shape) + Fore.RESET)\n",
    "    \n",
    "    params = train(x_train, t_train, x_test, t_test, initParams)\n",
    "\n",
    "    if not load_params or not os.path.exists(param_cache_file):\n",
    "        with open(param_cache_file, 'wb') as f:\n",
    "            pickle.dump(params, f, -1)\n",
    "            \n",
    "elif param_cache:\n",
    "    network = TwoLayerNet(input_size=784, hidden_sizes=hidden_sizes, output_size=10, initParams=param_cache, weight_decay_lambda=weight_decay_lambda)\n",
    "\n",
    "    print(\"Results: \" + Fore.YELLOW + str(np.argmax(network.predict(x_test), axis=1)) + Fore.RESET)\n",
    "    print(\"Expects: \" + Fore.GREEN + str(np.argmax(t_test, axis=1)))\n",
    "    \n",
    "else:\n",
    "    print(Fore.YELLOW + \"Please train params first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFggngb-Zdlh"
   },
   "outputs": [],
   "source": [
    "## Reference\n",
    "\n",
    "1. Saito Yasuhiro. Deep Learning from Scratch[M]. Japan: O'Reilly Japan, 2016.\n",
    "\n",
    "2. 管他叫大靖. (2021年05月24日). Softmax Loss 的推导及改进. 知乎专栏. (https://zhuanlan.zhihu.com/p/374018199).\n",
    "\n",
    "3. Khelifi Ahmed Aziz. Medium. Learn How to Write Markdown & LaTeX in The Jupyter Notebook (https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)\n",
    "\n",
    "3. Jay Gupta. Medium. Going beyond 99% — MNIST Handwritten Digits Recognition (https://towardsdatascience.com/going-beyond-99-mnist-handwritten-digits-recognition-cfff96337392)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyO/yw1/LLBQwwj444gf7I4K",
   "collapsed_sections": [
    "RjRQLf1HjZZY",
    "428IuPKjnPPa",
    "V4ikKQHFmTLT"
   ],
   "include_colab_link": true,
   "mount_file_id": "18B-Fujnr7uDhfyERZzWHTI3-31anw5OH",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
